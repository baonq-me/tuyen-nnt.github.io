
var documents = [{
    "id": 0,
    "url": "https://tuyen-nnt.github.io/404.html",
    "title": "404",
    "body": "404 Page not found!Please use the search bar from the bottom left or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://tuyen-nnt.github.io/about",
    "title": "Hello, My name is Tuyen (Marie Nguyen). Welcome to my page!",
    "body": "This website is a demonstration to see Memoirs Jekyll theme in action. The theme is compatible with Github pages, in fact even this demo itself is created with Github Pages and hosted with Github.  Get Memoirs for Jekyll → "
    }, {
    "id": 2,
    "url": "https://tuyen-nnt.github.io/authors",
    "title": "Authors",
    "body": "                                                                                                                                                                                    Tuyen:         Author of this blog, mostly about Technical which is the field I am interested in.                "
    }, {
    "id": 3,
    "url": "https://tuyen-nnt.github.io/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 4,
    "url": "https://tuyen-nnt.github.io/contact",
    "title": "Contact",
    "body": "  Please send your message to Tuyen Nguyen. We will reply as soon as possible!   "
    }, {
    "id": 5,
    "url": "https://tuyen-nnt.github.io/",
    "title": "Home",
    "body": "                                                                                               Tìm hiểu Network              :       1. Mô hình mạng 5 tầng Tầng 1: Physical:                                                                               Tuyen                 18 Oct 2021                                                                                                                           Tìm hiểu về Data Analysis              :       Bài viết này sẽ hướng dẫn các bước cơ bản: Tìm data phù hợp rồi collect chúng Đọc data trong môi trường dev Chuẩn bị phân tích bằng cách cleaning. . . :                                                                               Tuyen                 03 Sep 2021                                                                                                                           Tìm hiểu về giao thức HTTP              :       HTTP request:                                                                               Tuyen                 02 Sep 2021                                                                                                                           Setup Jekyll themes              :       Fork Jekyll theme &amp; Play:                                                                               Tuyen                 10 Aug 2021                                                                                                                           Python - Part 2              :       1. Dictionaries Là tập hợp các cặp key:value khi cần kết nối dữ liệu với nhau như 1 table để tra cứu nhanh và có thể chỉ ra unique keys. . . :                                                                               Tuyen                 02 Aug 2021                                                                                                                           Import Data - Part 2              :       1. Import + Load + Tạo HTTP/GET REQUEST Lưu file mềm xuống local:urlretrieve(url, 'filename. csv'):                                                                               Tuyen                 18 Jul 2021                                   &laquo;          1        2        3        4       &raquo; "
    }, {
    "id": 6,
    "url": "https://tuyen-nnt.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 7,
    "url": "https://tuyen-nnt.github.io/page2/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 8,
    "url": "https://tuyen-nnt.github.io/page3/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 9,
    "url": "https://tuyen-nnt.github.io/page4/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 10,
    "url": "https://tuyen-nnt.github.io/network/",
    "title": "Tìm hiểu Network",
    "body": "2021/10/18 - 1. Mô hình mạng 5 tầng:  Tầng 1: PhysicalVí dụ như dây cáp mạng, hay các đầu nối, tín hiệu gửi để kết nối các máy tính với nhau. Ngoài ra tầng này còn mô tả cách tín hiệu được gửi qua các kết nối vật lý này.  Tầng 2: Data LinkDùng để diễn giải các tín hiệu của tầng 1 một cách chung chung để các thiết bị mạng có thể giao tiếp. Có nhiều giao thức ở tầng này nhưng phổ biến nhất là Ethernet, có nhiệm vụ nhận dữ liệu và truyền đến các nodes trên cùng 1 mạng hoặc link. Tầng này nhận dữ liệu trên 1 đường liên kết đơn.  Tầng 3: Network (or Internet) Là tầng cho phép các mạng khác nhau liên kết với nhau thông qua các thiết bị router (bộ định tuyến).  Tập hợp các mạng kết nối với nhau thông qua router gọi là Internetwork (mạng liên kết). Mạng liên kết phổ biến nhất là Internet :D. Tầng này chịu trách nhiệm nhận dữ liệu được phân phối từ 1 tập hợp các mạng. Giao thức phổ biến nhất ở tầng này là giao thức IP (Internet protocol). Một phần mềm mạng gồm Client và Server:  Client gửi request Server thì gửi về responseMột node có thể chạy nhiều chương trình client hoặc server cùng lúc. Kiểu như ở client (server host) của bạn cùng thao tác nhiều chương trình cùng lúc để request (như mail, trình duyệt web), thì server ngoài cũng có thể chạy nhiều chương trình để trả về response cho bạn.  Tầng 4: TransportTrong khi tầng Network cung cấp dữ liệu giữa 2 node riêng lẻ thì tầng Transport có nhiệm vụ phân loại ra chương trình client hay server nào có nhiệm vụ nhận dữ liệu đó. Giao thức phổ biến nhất ở tầng này chính là TCP (Transmission Control Protocol). Người ta hay gọi theo cụm là TCP IP.  Nhưng TCP IP ko phải là tên gọi của 1 giao thức, nó chỉ là tên gọi chung kết hợp của giao thức ở tầng 3 và tầng 4. Mỗi giao thức đều có công dụng khác nhau. Ngoài ra có 1 giao thức thuộc tầng 4 transport cũng sử dụng giao thức IP tầng 3 để truyền tải dữ liệu. Đó là giao thức UDP (User Datagram Protocol). Nhưng giao thức này không đảm bảo độ tin cậy bằng TCP, mình sẽ nói thêm ở mục sau. Tóm lại, các giao thức ở tầng này hầu hết có nhiệm vụ đảm bảo dữ liệu được truyền tải đến “đúng” các ứng dụng đang chạy trên các nút đó.  Tầng 5: ApplicationGiao thức phổ biến ở tầng này là HTTP (cho phép chúng ta duyệt web), SMTP (gửi/nhận email), etc. . Tóm tắt: Giải thích 1 cách dễ hiểu hơn::  Tầng 1: Chiếc xe tải Tầng 2: Mô tả cách xe tải đi từ giao lộ này đến giao lộ kia.  Tầng 3: Xác định đường để đi từ địa chỉ A đến địa chỉ B để giao hàng.  Tầng 4: Đảm bảo là tài xế biết cách gõ cửa nhà để biết đến đúng địa chỉ nhà rồi và thông báo ra nhận gói tin.  Tầng 5: Chính là nội dung của gói hàng (dữ liệu) !Có thể bạn chưa biết?:  Ngoài model 5 tầng còn nhiều model khác. Nổi tiếng nhất là OSI model (7 tầng), khác với loại 5 tầng mà chúng ta học ở chỗ tầng 5 được tách ra 3 tầng.  TCP/IP truyền thống chỉ có 4 tầng vì Tầng 1 và tầng 2 được gộp làm 1. Về cơ bản cái truyền thống 4 tầng và 5 tầng hiện nay không khác nhau là mấy.  Chúng ta có thể tìm hiểu thêm về OSI Model tại đây:```https://www. sans. org/reading-room/whitepapers/standards/osi-model-overview-543https://en. wikipedia. org/wiki/OSI_model``` 2. Các thiết bị mạng cơ bản: Cable: Dùng để kết nối các thiết bị với nhau và cho phép truyền dữ liệu thông qua nó. Giúp tạo kết nối mạng đơn giữa 1 điểm với 1 điểm. Có 2 loại:  Copper cable: Cat3, Cat5, Cat5e, Cat6,…     Các cable đời mới hơn như Cat5e, Cat6 tốt hơn trong việc truyền dữ liệu nhiều hơn và chính xác hơn do chúng có thông số kỹ thuật nâng cao và lõi xoắn của chúng được cấu tạo khác đi để giảm sự xuyên âm (crosstalk- khi mà xung điện trên 1 dây được phát hiện trên 1 dây khác) giúp giảm tình trạng đầu nhận dữ liệu không hiểu được dữ liệu gây ra lỗi mạng.    Gửi giao tiếp dữ liệu nhị phân qua dây đồng bằng cách thay đổi điện áp giữa 2 phạm vi. Hệ thống ở đầu nhận dữ liệu sẽ dịch sự thay đổi điện áp này thành dữ liệu nhị phân 0-1. Sau đó từ dữ liệu 0-1 này sẽ được dịch sang các loại dữ liệu khác nhau.    Sử dụng áp điện để biểu diễn dữ liệu dưới dạng 0-1    Fiber optic     Sử dụng xung ánh sáng để biểu diễn dữ liệu dưới dạng 0-1.    Được dùng ưu tiên cho các môi trường có nhiều nhiễu điện từ.    Vận chuyển dữ liệu nhanh hơn, khoảng cách xa hơn mà khó làm mất dữ liệu tiềm ẩn hơn.    Đắt và dễ vỡ hơn.    Hub &amp; Switch: Là những thiết bị giúp kết nối các máy tính trong cùng 1 mạng, thường gọi là mạng LAN (mạng cục bộ) Hub: 1 thiết bị bất kỳ trong mạng lưới của hub khi truyền dữ liệu tới hub sẽ kết nối đến tất cả các máy tính khác mà cùng được kết nối với hub.  Các máy tính nhận dữ liệu có hệ thống riêng để xác định xem dữ liệu đó có đúng là dữ liệu mà nó có nhiệm vụ nhận hay không. Nếu không thì trả về không, nếu có thì thông báo nhận thành công. Cơ chế này gây ra nhiều tiếng ồn (noise) tạo ra cái gọi là collision domain. Collision domain:    Một phân đoạn mạng mà chỉ có 1 thiết bị có thể giao tiếp tại 1 thời điểm.     Nếu nhiều hệ thống (máy tính) cố gắng gởi dữ liệu cùng 1 lúc, các xung điện được gởi qua cáp (cable) có thể gây nhiễu lẫn nhau. Khiến cho các hệ thống phải chờ đợi một khoảng thời gian yên tĩnh trước khi cố gắng gửi dữ liệu lần nữa.   Điều này làm chậm truyền thông mạng nên hiện nay Hub không còn được dùng phổ biến. Switch: Mô hình hoạt động tương đối giống Hub, khác biệt ở chỗ:  Hub ở layer 1, Switch ở layer 2 (Data Link device) Do đó Switch có thể kiểm tra dữ liệu của giao thức Ethernet được gửi đi xung quanh network. =&gt; Switch có thể xác định hệ thống (máy tính) nào thuộc về dữ liệu đó mà chỉ gửi thẳng đến hệ thống đó thôi.  Điều này giúp loại bỏ hoàn toàn kích thước của collision domain trong network. Gíup giảm sự truyền tải lại và tăng thông lượng tổng thể. Router: Là thiết bị biết các forward dữ liệu giữa các mạng độc lập với nhau.     Hoạt động ở lớp thứ 3 (Network)     Cũng giống như switch, router có thể kiểm tra dữ liệu IP để xác định địa chỉ cần gửi dữ liệu đến.     Router chứa 1 số bảng nội bộ chứa các thông tin về định tuyến traffic (lưu lượng truy cập) giữa nhiều mạng khác nhau trên thế giới. Phổ biến là các router gia đình hoặc văn phòng nhỏ. Mục đích của các bộ định tuyến nhỏ này chỉ để lấy lưu lượng truy cập có nguồn gốc từ trong nhà hoặc văn phòng nhỏ sử dụng LAN và chuyển tiếp nó đến ISP (nhà cung cấp dịch vụ Internet)     ISP là một loại router phức tạp hơn nhiều, nó sẽ tiếp quản traffic từ router nhỏ. Router này gọi là router lõi, tạo thành xương sống của Internet, chịu trách nhiệm về cách gửi và nhận dữ liệu khắp thế giới mỗi ngày.     ISP tiếp nhận rất nhiều traffic và phải xử lý việc quyết định nơi nào là nơi gửi traffic này đến. Core router này thường có nhiều kết nối đến nhiều core router khác bằng giao thức BGP (Border Gateway Protocol) giúp chúng tìm hiểu xem đường nào là tối ưu nhất để chuyển lưu lượng truy cập đến.   Khi bạn mở trình duyệt truy cập đến 1 web server bất kỳ, traffic giữa máy tính và web server có thể đã đi qua hàng chục router khác nhau. Các Router là những hướng dẫn viên toàn cầu giúp đưa traffic đến đúng nơi. Server và Client: Nodes: là từ dùng để gọi chung cho các thiết bị trong network ở trên, có thể là máy tính, server, client hay router,… Server: là node có nhiệm vụ cung cấp data cho client, còn được dùng để chỉ mục đích chính của node trên mạng. Client: là node request data. Đôi khi 1 node vừa làm server vừa là client. Ví dụ như Email server, vừa cung cấp data về email cho client, mà vừa là 1 client gửi yêu cầu đến DNS server và được DNS trả data về. =&gt; Hầu hết các thiết bị không hoàn toàn là 1 server hay 1 client, mà đảm nhận 1 trong 2 vị trí trên ở 1 thời điểm.  3. Tầng 2 - Data Link Layer: Ethernet:  Là protocol mạng phổ biến nhất ở tầng 2 để gửi dữ liệu qua các liên kết riêng lẻ. (còn Wi-Fi thì dạng khác) Là phương tiện giúp trừu tượng hóa tầng 1 (vật lý hay phần cứng) để các tầng khác có thể dễ tiếp cận sử dụng.  Chịu trách nhiệm về các thông tin kết nối, liên kết thiết bị để các tầng trên dựa vào. Vì vậy, các tầng trên không cần quan tâm thiết bị được kết nối ra sau, mà chỉ quan tâm tầng dưới gửi dữ liệu gì để xử lý mà thôi.  Ethernet có 1 kỹ thuật là CSMA/CD (Carrier-sense multiple access with collision detection) giúp nhận biết “collision domain” sóng mang xung đột.  CSMA/CD được dùng để quyết định khi nào kênh giao tiếp đang rãnh rỗi trên phân đoạn mạng (network segment) và khi nào 1 thiết bị đang rãnh để truyền dữ liệu.      Cách hoạt động khá đơn giản, khi có xung đột thì lập tức dừng truyền dữ liệu của các thiết bị gặp xung đột và chờ 1 khoảng thời gian ngẫu nhiên giúp tránh vụ va chạm (random interval trước khi thử gửi lại, và không đồng thời).     Phân đoạn mạng của tất cả các thiết bị được kết nối sẽ có tất cả thông tin, dữ liệu đang trao đổi. Do đó, ta cần xác định thiết bị nào là nơi dữ liệu cần đến bằng MAC Address. MAC Addresses (Media Access Control Address):  Là địa chỉ để nhận diện toàn cầu được đính kèm với một network interface cá nhân.  Giúp nhận dạng các máy tính khác nhau.  Là một con số 48-bit thường được biểu diễn bởi 6 nhóm với 2 con số hệ thập lục phân cho mỗi nhóm.  Nghĩa là sẽ có tối đa 2^48 MAC address.      Hệ thập lục phân là cách biểu diễn các số sử dụng 16 ký tự.        Octet: là 1 cách để tham chiếu số MAC. Trong mạng máy tính, bất kỳ số nào có thể biểu diễn bằng 8 bits được gọi là Octet. =&gt; 2 chữ số thập lục phân có thể biểu diễn các số tương tự các số được biểu diễn bằng 8 bits.   03 Octets đầu tiên của địa chỉ MAC được gọi là OUI (Organizationally unique identifier), được gán cho nhà sản xuất phần cứng riêng lẻ. =&gt; Từ 3 octets đầu có thể giúp ta xác định được NSX.    03 Octets sau được chỉ định theo ý nhà sản xuất mong muốn với điều kiện để mỗi thiết bị sản xuất ra có 1 MAC address duy nhất.   Tóm lại, Ethernet dùng MAC address để đảm bảo dữ liệu mà nó gửi có thông tin thiết bị nguồn và thiết bị đích gửi đến. Bằng cách này, dù ở trong một phân đoạn mạng hoạt động như thể có collision domain đơn lẻ, thì các thiết bị luôn biết được khi nào dữ liệu thuộc về nó. Ethernet Frame:    Là 1 tập hợp các thông tin mang tính tổ chức cao, biểu diễn theo 1 thứ tự cụ thể.     Bằng Ethernet frame, network interface có tại tầng 1 có thể chuyển đổi chuỗi bit qua một liên kết để đưa vào trong data có ý nghĩa (decode)     Các phần trong frame là bắt buộc và có độ dài cố định.    Preamble: phần mở đầu, chia làm 7 bytes đệm + 1 byte SDF (dùng để làm dấu giới hạn khung bắt đầu). Được các network interface dùng để đồng bộ hóa các đồng hồ nội bộ mà chúng sử dụng nhằm điều chỉnh speed tại nơi mà chúng gởi dữ liệu. "
    }, {
    "id": 11,
    "url": "https://tuyen-nnt.github.io/data-analysis/",
    "title": "Tìm hiểu về Data Analysis",
    "body": "2021/09/03 - Bài viết này sẽ hướng dẫn các bước cơ bản:  Tìm data phù hợp rồi collect chúng Đọc data trong môi trường dev Chuẩn bị phân tích bằng cách cleaning và validation. 1. Reading data: Dùng pandas để đọc các format phổ biến như CSV, Excel, HDF5,… 2. Columns và Rows: ####df. shape  in ra (n, m) với n là số row và m là số column ####df. columns  in ra list các column ở định dạng string Dataframe giống như Dictionary khi variable name là key (tên cột) còn các giá trị trong row là values. Do đó bạn có thể select column dùng df[ key ]. Kết quả là 1 Series các value của cột đó. Dtype của nó là float64. **Lưu ý: NaN là là giá trị đặc biệt chỉ định value không hợp lệ hoặc bị thiếu. ** 3. Clean và Validate: Đây là bảng data chứa cân nặng của baby ta dùng trong các ví dụ sắp tới:  Đầu tiên ta rút trích mỗi cột mà ta muốn phân tích lưu vào 1 biến:pounds= df[ tên cột ] ounces= df[ tên cột ]  Ta xem có value gì xuất hiện trong cột mà ta muốn phân tích và mỗi value xuất hiện bao nhiêu lần:pounds. value_counts(). sort_index() Mặc định kết quả sort theo giá trị nào gặp nhiều nhất. Nên ta thêm sort_index() để nó sort theo giá trị. =&gt; Sau đó ta quay lại xem bảng data ban đầu để thấy sự hợp lý. Ta có thể kết luận data này đúng và chúng ta đang phân tích đúng. Series. Describe(): Ta cũng có thể dùng attribute describe để có bảng thống kê mean, phương sai, min và max rồi kết luận như trên. pounds. describe() Giải thích bảng trên:  50% : giá trị trung vị =7 mean: trung bình =8, do có chứa các giá trị đặc biệt ít gặp như quá cao hay quá thấp nên không có ý nghĩa lắm. Do vậy, trước khi đưa vào tính toán mean thực sự, ta phải thay thế những giá trị đặc biệt trên bằng NaN (thuộc thư viện numpy) để nó có nghĩa là data này bị mất đi giúp không ảnh hưởng đến số liệu phân tích chung của chúng ta. Series. Replace(): Tham số đầu tiên là list các giá trị ta muốn replace. Tham số thứ 2 là giá trị mà ta muốn được replace thành. Tham số thứ 3 tùy chọn là inplace=True, mặc định không đề cập thì là False. True nghĩa là thay thế series cũ, false là tạo mới series sau khi thay thế.   Trả về kiểu dữ liệu series. Nếu inplace=True thì không cần gán vào biến mới. Ta nhận thấy sau khi thay thế dữ liệu, mean() của series sẽ thay đổi. Arithmetic với Series: Tùy nhu cầu của bạn muốn tính toán hay combine giá trị của các cột với nhau. Ở ví dụ bài học này, ta sẽ cộng pounds và ounces lại với nhau. Đầu tiên ta phải đổi giá trị ounces thành pounds bằng cách chia 16 (cách đổi đơn vị cân nặng). Sau đó ta cộng lại. Kết quả trả về là 1 series là tổng giá trị của 2 series pounds và ounces. Đến đây ta có thể đưa kết luận giá trị trung bình của 1 đặc tính như cân nặng trong dataset bằng series. describe().  4. Filter và Visualize data: Histogram: Dùng để biểu thị tần suất xuất hiện của giá trị trong dataset. Để dùng biểu đồ này trong python, ta dùng thư viện matplotlib.     Tham số thứ 1 là series. Do histogram không nhận giá trị NaN nên chúng ta phải dùng hàm dropna() để loại bỏ nó trong series.     Tham số thứ 2 là bin. Số Bin nói với hist là nó muốn chia giá trị trên biểu đồ thành bao nhiêu interval (có thể hiểu là cột theo cân nặng đối với ví dụ) và đếm có bao nhiêu values trong dataset ứng với mỗi bin đó.  Ngoài ra còn các thông số tùy chọn khác xem thêm tại đây: https://matplotlib. org/stable/api/_as_gen/matplotlib. pyplot. hist. html Quan sát biểu đồ hình trên, ta có thể thấy tần suất baby có cân nặng nhẹ xuất hiện nhiều hơn.  Ta thấy điều này hợp lý vì trong dataset có chứa dữ liệu các em bé sinh non có số tuần mang thai &lt; 37 tuần. =&gt; Theo đó, ta tiếp tục rút trích dữ liệu của cột chứa số tuần của baby. Để xem những em bé nào sinh non, ta dùng Boolean Series. Boolean Series: Trả về series gồm các giá trị True hoặc False cho điều kiện mà ta áp dụng. Ta gán biểu thức gồm series của cột và điều kiện để trả về series chứa True và False: preterm = df[ tên cột tuần sinh ] &lt;37 Nếu ta tính tổng hay trung bình cho Boolean Series, python sẽ treat True=1 và False=0. =&gt; Do vậy kết quả của preterm. sum() là 3742. Đây là số lượng baby sinh non ứng với True. Và khi ta tính trung bình của Series, ta sẽ được tỉ lệ của True. Do đó preterm. mean() cho kết quả ~0. 39987. Nghĩ là khoảng 40% số baby có trong dataset này là baby sinh non. Filter: Ta có thể dùng Boolean Series để filter ra các Series giá trị của cột nào đó mà thỏa điều kiện mong muốn. Ví dụ để select ra các giá trị cân nặng trong Series birth_weight của các baby sinh non, ta dựa vào Boolean series preterm tương ứng các record True. Python sẽ đối chiếu cùng index với series chứa cân nặng của baby. Sau đó gán kết quả cho biến lưu series trả về chứa các giá trị cân nặng của em bé sinh non. Từ đó ta dễ dàng tính trung bình cân nặng của đối tượng này. preterm_weight = birth_weigth[preterm] Ngược lại, để tính toán trung bình cân nặng ta dùng dấu ~ trước Boolean series để lấy giá trị ngược lại là False. Kết quả:  Nhận xét: Trung bình cân nặng của em bé thường sẽ &gt; hơn em bé sinh non, và điều này hiển nhiên hợp lý. Nâng cao hơn, ta có thể sử dụng kết hợp &gt;2 boolean series để filter. Khi đó ta sẽ cần dùng đến Logical Operators AND hoặc OR. birth_weight[A &amp; B]	# both truebirth_weight[A | B]	# either or both trueResampling: Cuối cùng trước khi có thể trả lời câu hỏi cho set dữ liệu, ta cần thực hiện Resampling. Nói sơ về Sampling (lấy mẫu) trước. Sampling là quá trình chọn ra một tập con của một quần thể với mục tiêu đánh giá các tính chất của quần thể đó. Cách thức lấy mẫu phụ thuộc trực tiếp vào mục tiêu đánh giá của chúng ta, do đó sampling nằm gần ranh giới giữa việc quan sát khách quan và việc thực hiện các thực nghiệm mang tính chủ quan. Một số khía cạnh chúng ta cần cân nhắc khi lấy mẫu dữ liệu bao gồm:  Mục tiêu. Tính chất của quần thể mà chúng ta muốn khảo sát đánh giá.  Quần thể. Phạm vi khảo sát dựa trên lý thuyết.  Tiêu chí lựa chọn. Các nguyên tắc cho việc chấp nhận / loại bỏ các quan sát.  Kích thước mẫu. Số lượng các quan sát được thu nhận trong mẫu. =&gt; Resampling datacó ý nghĩa là cần khảo sát trên mẫu dữ liệu ta thu được nhiều lần để đánh giá độ chắc chắn cho các ước tính. Có hai phương pháp resampling thường được sử dụng là bootstrap và k-fold cross-validation:  Bootstrap. Các mẫu được lấy ra từ dataset một cách ngẫu nhiên, cho phép một mẫu được xuất hiện nhiều hơn một lần.  k-fold Cross-Validation. Dataset được chia thành k nhóm, mỗi nhóm sẽ được sử dụng để đánh giá 1 lần5. Probability mass functions (PMF): PMF Class: Ngoài histogram, ta có thể dùng PMF để quan sát tần số xuất hiện của từng giá trị trong dataset. PMF Class làm việc dựa trên Pandas Series và cung cấp 1 số functions không có trong Pandas. Tham số đầu tiên có thể là các loại sequence bất kỳ. pmf_educ = Pmf(educ, normalize=False)*educ ở đây là Series object  Kết quả trả về 1 PMF object với giá trị ở bên trái và count số lần xuất hiện trong tập dataset ở bên phải. Để lookup tần suất cho giá trị ở bên trái, ta chỉ cần dùng dấu ngoặc vuông:pmf_educ[12] Tuy nhiên thông thường khi cần dùng đến PMF thì thường ta muốn biết tỉ lệ xuất hiện của giá trị hơn là đếm Lúc này ta chỉ cần set tham số thứ 2 là normalize = True. Khi đó cột giá trị bên phải trả về tỉ lệ và tổng cột sẽ =1. Nếu muốn biết % ta chỉ cần *100 là được. Cách lookup cho 1 giá trị bất kỳ cũng tương tự trên.  PMF Bar chart: PMF có method riêng để hiển thị biểu đồ tần suất. Tùy ta muốn hiển thị tần suất theo tỉ lệ hay count thì ta dùng method lên biến lưu series ở bước trên.  So sánh Histogram và PMF: Tùy trường hợp nhưng trong ví dụ hình trên ta nhận xét:  PMF show tất cả unique value giúp ta thấy rõ chính xác peak của data ở đâu.  Histogram đặt giá trị theo bin nên làm mập mờ các chi tiết quan trọng, như việc ta không thấy peak nằm ở giá trị 12, 14 và 16. 6. Cumulative distribution functions (CDF): CDF Class: Ngoài PMF, còn các cách khác để thể hiện distribution đó là CDF. CDF là cách hay để visualize và so sánh distribution của giá trị. CDF cách hoạt động gần giống như PMF, khác nhau ở chỗ:  PMF cho ra tỉ lệ của 1 giá trị trong series từ 0 đến 1.  CDF cho ra tỉ lệ xuất hiện của các giá trị &lt;= giá trị đang tính toán từ 0 đến 1 (percentile). cdf_educ = Cdf(educ, normalize=False)Xem ví dụ sau: Vẽ biểu đồ plot dùng CDF: Ta chỉ cần dùng class Cdf() và input tham số là sequence của data mà ta muốn biểu thị tần suất. Ở hình dưới là “tuổi”: Đặc biệt, cdf có thể được sử dụng như 1 function với input là 1 giá trị cụ thể (biến số nguyên). q=51p=cdf(q)print(p) Kết quả cho ra 0. 66. Nghĩa là có 66% số người có tuổi &lt;=51. Inverse CDF: CDF là 1 function đảo ngược. Nghĩa là bạn có thể từ giá trị probability (tỉ lệ) mà tra ngược lại giá trị tuổi bằng cách dùng cdf. inverse(giá trị tỉ lệ) Từ hình trên có thể nói rằng, tuổi 30 là percentile thứ 25 của distribution này. (do p=0,25) Nói sơ thêm về percentile. Khoảng cách từ 25th đến 75th percentile gọi là interquartile range (IQR), nó giúp đo lường sự trải rộng của distribution. Do vậy IQR tương tự như phương sai (variance) hoặc độ lệch chuẩn (standard deviation). Vì IQR được tính toán dựa trên percentiles nên nó sẽ không bị loại đi những giá trị cực hoặc ngoài rìa (cách mà phương sai làm). Do đó IQR “mạnh mẽ” hơn phương sai, nghĩa là nó vẫn làm tốt công việc của nó dù có giá trị lỗi hoặc giá trị cực trong tập data. Giá trị cực (extreme value) là gì? Là những giá trị khi xuất hiện sẽ ảnh hưởng lớn đến sự thay đổi về xu hướng hội tụ (độ chụm, độ chính xác) kết quả tính toán chung của tập các số như các phép tính trung bình cộng, trung bình nhân,… 7. So sánh Distributions: Ta có thể dùng PMF hoặc CDF để plot rồi visualize và phân tích. Tuy nhiên với CDF ta sẽ có cái nhìn rõ ràng, không bị nhiễu và biểu đồ đường (line chart) trông sẽ mượt hơn nhiều. Ta ví dụ tập dataset là income của cư dân trước và sau 1995: Nếu dùng PMF : Kết quả chart: Nếu dùng CDF (khuyến khích) Kết quả chart: Nhận xét data:  Dưới 300000$ thì income hầu như không thay đổi trước và sau 1995. Đường màu cam lệch sang phải ở mốc income 100000-150000$ nghĩa là income sau 1995 của những người có thu nhập cao có xu hướng tăng lên. "
    }, {
    "id": 12,
    "url": "https://tuyen-nnt.github.io/http/",
    "title": "Tìm hiểu về giao thức HTTP",
    "body": "2021/09/02 - HTTP request: HTTP là giao thức (protocol) giúp browser (client) giao tiếp với web server (server). Khi bạn truy cập 1 trang web nghĩa là bạn đang gửi 1 HTTP request đến web server. Trong HTTP request sẽ chứa Header và Body. Port mặc định của các giao thức HTTP hiện nay trên các trang web:  HTTP: 80 HTTPS: 443Dòng bắt đầu của HTTP request (như 1 message) chứa 3 phần sau::  Version của protocol: HTTP/1. 1 HTTP Method như GET, POST, PUT,HEAD, OPTION, DELETE Path của request : thông thường là 1 URL và format của nó phụ thuộc vào HTTP method. Ví dụ:Dùng đường dẫn tuyệt đối, phổ biến và thường dùng với GET, POST, HEAD, OPTIONS:  POST / HTTP/1. 1 GET /background. png HTTP/1. 0 HEAD /test. html?query=alibaba HTTP/1. 1 OPTIONS /anypage. html HTTP/1. 0Dùng đường dẫn hoàn chỉnh khi cần kết nối với proxy thông qua method GET:  GET https://developer. mozilla. org/en-US/docs/Web/HTTP/Messages HTTP/1. 1Chỉ dùng authority form khi cần setup HTTP tunnel (domain name : port) bằng method CONNECT:  CONNECT developer. mozilla. org:80 HTTP/1. 1Dùng dấu '*' khi muốn đường dẫn đại diện cho toàn bộ máy chủ thông qua method OPTIONS:  OPTIONS * HTTP/1. 1Header chứa các thông tin của request và hầu hết ở dưới dạng key:value. : Header của request được chia thành vài group chính:  General header: apply cho toàn bộ message Response header: chỉ định thêm cho request bằng cách sửa đổi 1 số thông số Representation headers: như Content-Type để mô tả format của data gửi lên server và cho biết nếu data đó có apply encoding nào không (chỉ có phần này khi request có Body)Header key có thể được set tùy chỉnh valueTrong POST method, bạn tùy chỉnh theo mẫu sau: # Use case: Set the output type as JSON and json. dumps your output. # Set_default_headers in a parent class called RESTRequestHandler. If you want just one request that is returning JSON you can set the headers in the post call. class strest(tornado. web. RequestHandler):  def set_default_headers(self):    self. set_header( Content-Type , 'application/json')  def post(self):    value = self. get_argument('key')    cbtp = cbt. main(value)    r = json. dumps({'cbtp': cbtp})    self. write(r)Tham khảo thêm các key info trong header tại: https://developer. mozilla. org/en-US/docs/Web/HTTP/Headers Body: Không phải method nào cũng cần Body, hầu như GET, HEAD, DELETE, OPTIONS ít cần. Thông thường khi cần gửi request update thông tin gì đó lên server thì người ta dùng method POST request (chứa dữ liệu HTML form). Body chia thành 2 loại :  Single-resource body: chứa 1 loại file duy nhất, được định nghĩa bởi Content-Type và Content-Length (trong header).  Multiple-resource body : chứa multipart data, mỗi part chứa một số thông tin khác nhau. Các part được phân tác bởi dấu -- trong header ở phần Content-Type. Loại này thường được sử dụng với HTML form. Content-Type: multipart/form-data; boundary=aBoundaryString(other headers associated with the multipart document as a whole)--aBoundaryStringContent-Disposition: form-data; name= myFile ; filename= img. jpg Content-Type: image/jpeg(data)--aBoundaryStringContent-Disposition: form-data; name= myField (data)--aBoundaryString(more subparts)--aBoundaryString--HTTP Method:: GET: Dữ liệu request sẽ hiển thị trên URL nên không bảo mật. Phù hợp khi cần download về dữ liệu gì đó vì nó truy xuất nhanh khi dữ liệu không hoặc ít bị thay đổi. Dữ liệu của phương thức này gửi đi thì hiện trên thanh địa chỉ (URL) của trình duyệt. /test/demo_form. php?user=itplus&amp;password=admin Đặc điểm:  HTTP GET có thể được cache bởi trình duyệt HTTP GET có thể duy trì bởi lịch sử đó cũng là lý do mà người dùng có thê bookmark được.  HTTP GET không được sử dụng nếu trong form có các dữ liệu nhạy cảm như là password, tài khoản ngân hàng HTTP GET bị giới hạn số trường độ dài data gửi điPOST: Dữ liệu gửi đi (khi request) sẽ không bị hiển thị trên thanh URL vì nó đã được encode (mã hóa) nên độ bảo mật cao. Đặc điểm:  HTTP POST không cache bởi trình duyệt HTTP POST không thể duy trì bởi lịch sử đó cũng là lý do mà người dùng không thê bookmark HTTP POST được.  HTTP POST không giới hạn dữ liệu gửi điPhân biệt POST và GET: Điểm chung: là các HTTP method dùng để trao đổi dữ liệu giữa client và server. Điểm khác nhau:  POST: Bảo mật hơn GET vì dữ liệu được gửi GET: Dữ liệu được gửi tường minh, chúng ta có thể nhìn thấy trên URL, đây là lý do khiến nó không bảo mật so với POST.  GET thực thi nhanh hơn POST vì những dữ liệu gửi đi luôn được webbrowser cached lại.  Khi dùng phương thức POST thì server luôn thực thi và trả về kết quả cho client, còn phương thức GET ứng với cùng một yêu cầu đó webbrowser sẽ xem trong cached có kết quả tương ứng với yêu cầu đó không và trả về ngay không cần phải thực thi các yêu cầu đó ở phía server.  Đối với những dữ liệu luôn được thay đổi thì chúng ta nên sử dụng phương thức POST, còn dữ liệu ít thay đổi chúng ta dùng phương thức GET để truy xuất và xử lý nhanh hơn. HEAD: Trả về response là header của request. HTTP response: Dòng đầu phần response chứa 3 thông tin:  Protocol version Status code: mã trạng thái trả về để biết request thành công hay thất bại Status text: text để giải thích cho codeHeader của response được chia thành vài group chính:  General header: apply cho toàn bộ message Response header: chỉ định thêm cho request bằng cách sửa đổi 1 số thông số.  Representation headers: như Content-Type để mô tả format của data trong response message và cho biết nếu data có apply encoding nào không (chỉ có phần này khi request có Body). Body: Không phải response nào cũng có body khi mà response đã đáp ứng đủ request mà không cần payload gì thêm. VÍ dụ như các status code như 201 Created hoặc 204 No Content. Body response có thể chia thành 3 loại :  Single-resource body: chứa 1 loại file duy nhất, được định nghĩa bởi Content-Type và Content-Length (trong header).  Single-resource body: chứa 1 loại file duy nhất, không biết độ dài, được mã hóa bằng các khối với key Transfer-Encoding : chunked.  Multiple-resource body : chứa multipart section, mỗi section chứa một số thông tin khác nhau. Loại này ít gặp. HTTP Location: Trong phần này có liên quan đến Redirection Ref: https://developer. mozilla. org/en-US/docs/Web/HTTP/Messageshttps://developer. ibm. com/articles/what-is-curl-command/ "
    }, {
    "id": 13,
    "url": "https://tuyen-nnt.github.io/Jekyll-install/",
    "title": "Setup Jekyll themes",
    "body": "2021/08/10 - Fork Jekyll theme &amp; Play: Đây là theme blog mình đang sử dụng: https://github. com/tuyen-nnt/jekyll-theme-memoirs Các bước build Jekyll cho blog này: https://bootstrapstarter. com/jekyll-theme-memoirs/(Tìm hiểu thêm tại: https://jekyllrb. com/docs/)  B1: git clone https://github. com/wowthemesnet/jekyll-theme-memoirs. git B2: Tải Ruby https://www. ruby-lang. org/en/documentation/installation/ B3: cd vào thư mục theme rồi gem install bundler B4: bundle installKết quả build thành công:  B5: Sửa lại _config. yml theo blog của mình B6: bundle exec jekyll serve --watch B7: Xem web blog tại http://127. 0. 0. 1:4000/jekyll-theme-memoirs (nếu folder vẫn giữ tên cũ) B8: Thêm blogs định dạng . md vào folder _posts. Trước mỗi bài viết sẽ có ô YAML là định dạng chung, bạn chỉ cần điền vào thông tin của mình là được (nhưng vẫn giữ form nhé). Chi tiết xem link các bước thực hiện. Tìm hiểu về Bundler install: https://bundler. io/ Tìm hiểu về Gemfile: https://bundler. io/gemfile. html "
    }, {
    "id": 14,
    "url": "https://tuyen-nnt.github.io/python-intermediate/",
    "title": "Python - Part 2",
    "body": "2021/08/02 - 1. Dictionaries: Là tập hợp các cặp key:value khi cần kết nối dữ liệu với nhau như 1 table để tra cứu nhanh và có thể chỉ ra unique keys khi tra cứu, thay vì nối 2 list lại để lấy index rồi tra cứu value.    So sánh giữa list và dict:     Cú pháp tạo dict như sau:  my_dict = {	key:value,	key:value}    Access dict như sau:my_dict['key'] =&gt; cho ra value của key đó.     Thêm key:value vào dict:world[ sealand ] = 0. 25     Check xem dict đã được thêm key ở trên vào chưa: sealand in world =&gt; trả về True/False. Với ‘seadland’ là key và word là tên dict.     Cập nhật lại giá trị cho key sealand:world['sealand'] = 0. 28Vì key trong dict là unique nên Python hiểu là bạn muốn thay đổi giá trị chứ không phải tạo mới cặp key:value.     Xóa cặp key:value:del(world['sealand'])     Dict trong dict:Cũng như list có thể chứa list trong list. Xem ví dụ:   # Dictionary of dictionarieseurope = { 'spain': { 'capital':'madrid', 'population':46. 77 },     'france': { 'capital':'paris', 'population':66. 03 },     'germany': { 'capital':'berlin', 'population':80. 62 },     'norway': { 'capital':'oslo', 'population':5. 084 } }  Để access giá trị của dict, ta sẽ dùng dấu [] như trong array:  europe['spain']['population'] =&gt; Vậy để add thêm cặp key:dict vào trong dict trên thì làm thế nào? # Create sub-dictionary datadata = {  'capital':'rome',  'population':59. 83}# Add data to europe under key 'italy'europe['italy'] = dataTa chỉ cần tách ra tạo dict phụ trước và gán nó vào biến lưu object value. Sau đó ta thêm cặp key:value vào dict như bình thường. Lưu ý:  Keys không được trùng nhau trong 1 dict, vì nếu trùng, nó sẽ chỉ lấy giá trị cuối cùng.  Keys phải là immutable object (không đổi), còn list thì mutable nên list cũng không được là key trong dict. 2. Pandas: Tabular dataset trong Pythonrow = observations column = variable  Để làm việc với dạng data này thì cần cấu trúc dạng chữ nhật. =&gt; 2D Numpy array =&gt; Nhưng với các dữ liệu có nhiều thông tin với nhiều datatype khác nhau như str, float,… thì Numpy chưa hiệu quả. Vậy nên pandas package chính là solution và quen thuộc trong Data science.  Nó được build dựa trên Numpy.  Là tool ở cấp độ cao trong thao tác với dữ liệu.  Pandas lưu dữ liệu bảng trong object gọi là Dataframe. Cách tạo dataframe từ dictionary: import pandas as pdbrics = pd. DataFrame(dict) Để tạo index cho observations trong df, ta dùng attribute index và gán 1 list với thứ tự chính xác các index mong muốn: brics. index = [. . . ,. . . ] CSV fileNhưng thực tế trong Data science, ta phải đối mặt với lượng data khổng lồ tùy trường hợp cụ thể, nên thông thường ta không tự tạo dataframe. Giả sử các data đến từ file có . csv viết tắt của comma separated values.  Để import vào môi trường Python ta dùng cú pháp:brics = pd. read_csv( path/to/brics. csv ) Tuy nhiên đối với file có index, khi import vào thì cột index đầu tiên sẽ bị ngầm hiểu là cột đầu của dữ liệu chính. Để tránh điều này, ta phải thêm argument index_col=0, kết quả:  Để thay đổi index tự động từ 0-n bằng label được định nghĩa trong 1 list tạo riêng tự chọn, ta dùng cú pháp:```  Definition of row_labels  row_labels = [‘US’, ‘AUS’, ‘JPN’, ‘IN’, ‘RU’, ‘MOR’, ‘EG’]  Specify row labels of carscars. index = row_labels ##### Cách để access DataframeTa input vào label hoặc index của column hoặc row. Ví dụ cụ thể ta có df là cars, xem cú pháp dưới đây: * Access COLUMN (cột):	* Dùng [] : 	- Nếu muốn output là Series object: ``cars['country', . . . ]``	- Nếu muốn output là Dataframe, dùng double ngoặc vuông:``cars[['country', . . . ]]``&gt; Nhìn ở góc khác, ta đang input vào ngoặc vuông 1 list chứa column labels. 	* Dùng ``loc`` (chọn 1 phần data dựa trên label-based) hoặc ``iloc`` (chọn data dựa trên integer position-based)	- cars. loc['country', . . . ] hoặc cars. loc[['country', . . . ]] 	- car. iloc[0, . . . , . . . ] hoặc car. iloc[[0, . . . , . . . ]]	* Access ROW (quan sát)		* Chỉ có cách là dùng [] nhưng input vào số:	- Nếu muốn lấy row từ index 1 đến 3:``cars[1:4]`` 	* Dùng loc hoặc iloc và input vào index của row thay vì tên cột như truy cập vào column. * Access ROWs &amp; COLUMNs bất kì:	* Sử dụng loc và iloc tiện lợi:	- Ta chỉ cần đặt vào label của row và column trong loc hoặc iloc theo thứ tự ``row, column``. 	- Nếu chọn nhiều hơn 1 label trong row hoặc column, ta biến argument row hoặc column thành list. Xem	 hình ví dụ:	![](/assets/images/loc-iloc. png)	&gt; Nhận xét: - Dấu ngoặc vuông``[]`` có giới hạn chức năng và lý tưởng nhất là sử dụng trong 2D Numpy array để access value dễ dàng nhất. 	- Nếu muốn dấu``[]``có thể mở rộng khả năng access value trong pandas như dấu``[]``trong 2D Numpy array, thì ta cần sử dụng ``loc`` và ``iloc``. ![](/assets/images/iloc-loc. png)		##### Filter dataframe* Bước 1: Access cột trả về series object. * Bước 2: Xác định điều kiện filter và trả về Boolean Series. Nếu &gt; 2 điều kiện thì phải sử dụng Numpy variants của toán tử and, or, not. * Bước 3: Dùng Boolean Series là kết quả của bước 1 làm input trong dấu ngoặc vuông của Dataframe. Kết quả trả về các record thỏa điều kiện. &gt; You'll want to build up a boolean Series, that you can then use to subset the cars DataFrame to select certain observations. If you want to do this in a one-liner, that's perfectly fine!#### 3. LOOP##### WHILE##### FOR for var in seq :	expression Trong đó ``var`` là biến bất kỳ có thể đặt tên sao cũng được. Python dùng nó để quét lần lượt cái phần tử trong ``seq``. FOR còn dùng để lặp từng char trong string. ![](/assets/images/string-loop. png)* enumerate() : cung cấp 2 giá trị cho mỗi lần lặp gồm ``index`` và ``value (giá trị)``. ![](/assets/images/enumerate-for. png)Mỗi data structure sẽ có cách loop các nhau và cách định nghĩa sequence khác nhau (seq). Cụ thể các bạn xem dưới đây nhé:##### Loop với List của Lists * Nếu list mà bạn cần lặp là list của list, thì dùng cách như sau:house list of listshouse = [[“hallway”, 11. 25],     [“kitchen”, 18. 0],     [“living room”, 20. 0],     [“bedroom”, 10. 75],     [“bathroom”, 9. 50]] Build a for loop from scratchx quét từng list trong list, dùng [] để truy cập phần tử của sub-listfor x in house :  print(“the “ + x[0] + “ is “ + str(x[1]) + “ sqm”)``` Loop với DictionarySử dụng method items() : for key, val in my_dict. items() : Loop với Numpy arraySử dụng function np. nditer(my_array) đặc biệt là với 2D array. for val in np. nditer(my_array) :    Với 1D array ta có thể sử dụng loop thông thường, nhưng với 2D thì nó sẽ in ra 2D array thay vì ra các giá trị cần lấy trong loop.     Dùng nditer sẽ giúp in ra từng giá trị từ trái sang phải từ trên xuống dưới của 2D array.  Pandas DataFrameiterrows() : Trong mỗi lần lặp, method này sẽ generate ra 2 giá trị:  Label của row (nếu ko có thì là index tự động) Data của row (là Pandas Series có index/label là tên cột - còn gọi là fieldname) Để loop in ra giá trị của cột mong muốn cho mỗi lần lặp, ta chỉ cần: print(row[“tên cột”])  Thêm cột vào Dataframe bằng loop:Ví dụ ta muốn thêm cột tính độ dài của cột “country”:brics. loc[lab,  tên cột mới ] = len(row[ country ])  Nhận xét: Cách này tốt trong trường hợp ít record. Vì ta đang tạo ra Series object cho mỗi vòng lặp và nó sẽ không hiệu quả với các dataset khổng lồ, thậm chí gây ra vấn đề khi xử lý dữ liệu. Vậy nên, cách tốt nhất là ta sử dụng function apply(tên function) cho mỗi cột mà ta muốn tính toán rồi gán vào cột mới trong dataframe: brics[ cột mới ] = brics[ country ]. apply(len)  Cách hoạt động: Function apply() sẽ gọi function len() mà mỗi giá trị của cột country sẽ là input để tính độ dài từng country. Kết quả trả về là 1 array mà chúng ta có thể dễ dàng lưu thành cột mới trong Dataframe. "
    }, {
    "id": 15,
    "url": "https://tuyen-nnt.github.io/import-data-medium/",
    "title": "Import Data - Part 2",
    "body": "2021/07/18 - 1. Import + Load + Tạo HTTP/GET REQUEST:  Lưu file mềm xuống local:urlretrieve(url, 'filename. csv')Trước tiên phải from urllib. request import urlretrieve  Mở và đọc file mềm trên web:df = pd. read_csv(url, sep=';')   Show các dòng đầu tiên của df:print(df. head())   URL (Uniform/Universal Resource Locator)phần lớn là các địa chỉ web, ngoài ra còn là FTP (file transfer protocol)URL gồm 2 phần:  Protocol Identifier : http hoặc https   Tên resource: datacamp. com=&gt; tạo thành 1 địa chỉ web     HTTP (HyperText Transfer Protocol)Là protocol ứng dụng cho các hệ thống thông tin phân tán, cộng tác và siêu phương tiện, nền tảng giao tiếp dữ liệu cho WWW. HTTPS - có độ an toàn bảo mật cao hơn HTTP   Mỗi khi truy cập vào 1 trang web nghĩa là bạn đang gửi 1 HTTP request cho 1 server. Request này được gọi là GET request, đây là loại request phổ biến nhất.  urlretrieve : gửi GET request và lưu dữ liệu xuống local máy  HTML (HyperText Markup Language)2. Các cách gửi GET request::  Cách 1: sử dụng urllib. request=&gt; from urllib. request import urlopen, RequestMột số functions của package urllib. request  request = Request(url) : đóng gói GET request response = urlopen(request) : gửi request và catch phản hồi =&gt; trả về HTTP response object có tích hợp method read() và close() html = response. read() : trả về HTML định dạng string   response. close(): dùng xong nhớ đóng lại   Cách 2: rất phổ biến, sử dụng package requestsCho phép gửi HTTP request có tổ chức mà ko cần làm thủ công requests. get(url) : sau khi import package requests, hàm request. get() sẽ đóng gói request thông qua url, gửi request đi và nhận lại phản hồi và lưu vào biến r.  Ở đây hàm request. get() sẽ làm nhiệm vụ của Request(url) và urlopen(request đã đóng gói)của cách 1 r. text : r là biến lưu response của hàm trên, sử dụng method . text cho response để chuyển HTML của url sang dạng string. 3. Scraping web trong Python: HTML là sự kết hợp của data có cấu trúc và không cấu trúc.  Hàm BeautifulSoup() có tác dụng parse và trích xuất data từ HTML, và làm cho các tag được biểu diễn đẹp hơn. Cách sử dụng: from bs4 import BeautifulSoup Sau khi gửi nhận phản hồi của GET request, ta được file html như trên. Sau đó, ta dùng hàm BeautifulSoup để extract các data có cấu trúc của file html, lưu kết quả là một object vào một biến mới. Kết quả của hàm BeautifulSoupcó tích hợp hàm . prettify() để làm đẹp kết quả. soup = BeautifulSoup(html_doc)print(soup. prettify())    Các hàm khác có thể dùng sau khi parse và nhận kết quả từ BeautifulSoup:      soup. title() : trích title của file html   soup. get_text(): trích tất cả text của file html   soup. find_all() : tìm tất cả các data theo điều kiện hoặc tag nào đó.  Ví dụ:     for link in soup. find_all('a'): print(link. get('href')    hoặc      for link in a_tags: print(link. get('href'))      =&gt; Ở đây, ta sử dụng vòng lặp for kết hợp hàm . find_all() extract data nằm trong tag &lt;a&gt; của file html và in ra từng link của mỗi dòng tìm được, ta cũng có thể lưu soup. find_all() vào một biến nào đó. Hàm link. get('href') dùng để extract giá trị link của attribute href trong tag &lt;a&gt; 4. Load và khám phá file JSON: FIle JSON nằm ở local Bước 1: Tạo connection với file JSON trong local và load file  with open( tên file. json ) as json_file : json_data = json. load(json_file)  Ở đây json_data là 1 object dictionary, ta check bằng type(json_data) ra kết quả dict   Bước 2: Sử dụng vòng lặp for để in cặp key-value ra  for k in json_data. keys(): print(k + ': ', json_data[k])  Từ object dictionary trên, ta dùng àm . keys để truy cập vào keys của file và dùng cú pháp dictionary[key] để truy cập vào value.  5. APIs và tương tác cơ bản: API là gì? Là một bộ protocols và routines để xây dựng và tương tác với phần mềm Một tập hợp code cho phép 02 chương trình phần mềm giao tiếp với nhauVí dụ nếu muốn stream data của Twitter thì ta cùng API của Twitter.  Thông thường data thường được lấy về từ APIs ở định dạng JSON. URL có gì và làm thể nào để nó biết pull data từ API về?url = 'http://www. omdbapi. com/?t=hackers'  http - dấu hiệu là ta đang tạo 1 HTTP request www. omdbapi. com - nghĩa là ta đang query OMDB API ?t=hackers     Đây gọi là Query String   Không có quy ước và không buộc có trong đường dẫn   Sau dấu ? là phần query. Theo document trên trang chủ OMDB API thì có nghĩ là ta đang muốn trả về data của bộ phim có title (t) ‘Hackers’. Cụ thể xem phần Usage + Parameters.    # Import packageimport requests# Assign URL to variable: urlurl = 'https://en. wikipedia. org/w/api. php?action=query&amp;prop=extracts&amp;format=json&amp;exintro=&amp;titles=pizza'# Package the request, send the request and catch the response: rr = requests. get(url)# Decode the JSON data into a dictionary: json_datajson_data = r. json()print(json_data)# Print the Wikipedia page extractpizza_extract = json_data['query']['pages']['24768']['extract']print(pizza_extract)Ở 2 dòng code cuối, để biết tại sao code như thế, ta truy cập url trên browser, nó sẽ hiện ra các tab. Ta muốn extract data từ api của url thì ta mở từ tab query &gt; pages &gt; 24768 &gt; extract thì sẽ nhận được data từ api đó. Load và khám phá Twitter data# Import packageimport json# String of path to file: tweets_data_pathtweets_data_path = 'tweets. txt'# Initialize empty list to store tweets: tweets_datatweets_data = []# Open connection to filetweets_file = open(tweets_data_path,  r )# Read in tweets and store in list: tweets_datafor line in tweets_file:  tweet = json. loads(line)  tweets_data. append(tweet)# Close connection to filetweets_file. close()# Print the keys of the first tweet dictprint(tweets_data[0]. keys()) Đầu tiên ta gán đường dẫn tên file chứa Twitter data ở local máy vào 1 biến.  Tiếp theo ta tạo 1 mảng rỗng để chứa mỗi dòng tweet là 1 phần tử trong mảng.  Sau đó ta mở connection đến file local đó thông qua dường dẫn tweets_data_path và lưu vào tweets_file.  Tiếp theo ta dùng vòng lặp for để đọc từng dòng của tweets_file:     Dùng hàm json. load(line) để load từng dòng lưu vào biến tweet, để dùng hàm trên phải import json package         Note: mỗi lần load line để lưu vào tweet là một dictionary.           Sử dụng biến mảng tweets_data kết hợp hàm append(tweet) để add thêm phần tử dictionary mới (hay còn gọi là tweet) vào mảng.    In ra tất cả các keys của phần tử đầu tiên (tweet hay dict) của mảng.    Đưa mảng vào Dataframe sử dụng package pandas để phân tích# Import packageimport pandas as pd# Build DataFrame of tweet texts and languagesdf = pd. DataFrame(tweets_data, columns=['text','lang']) # Print head of DataFrameprint(df. head()) Hàm pd. Dataframe() cần 2 tham số là data và column để xây dựng df     Tham số đầu có thể là mảng, dict hoặc dataframe   Tham số thứ 2 là column label, nếu không có label thì dùng RangeIndex(0,1,2,…n). Nếu có label trong data như ví dụ trên, ta chỉ cần columns=['text','lang'] để chọn label cho column muốn rút giá trị. Ở ví dụ trên ta sẽ tạo 2 cột text và lang.    Streaming# Initialize Stream listenerl = MyStreamListener()# Create your Stream object with authenticationstream = tweepy. Stream(auth, l)# Filter Twitter Streams to capture data by the keywords:stream. filter(track=['clinton', 'trump', 'sanders', 'cruz'])   Class MyStreamListener() được khai báo sẵn tại đây: https://gist. github. com/hugobowne/18f1c0c0709ed1a52dc5bcd462ac69f4     Ta tạo object streambằng cách đưa vào hàm tweepy. Stream() athentication handler auth và object l - stream listener trên.     Object stream có tích hợp hàm . filter(), trong hàm này có attribute track=[] là list chứa các keyword mà ban muốn filter.  Phân tích data cơ bảnimport redef word_in_text(word, text):  word = word. lower()  text = text. lower()  match = re. search(word, text)  if match:    return True  return False Ở trên ta có hàm word_in_text() để đếm số lượng tweet chứa keyword. Nhưng ở đây chúng ta chưa đếm, mà chỉ đưa kết quả nếu True sẽ +1 vào biến đếm ở bước tiếp theo. # Initialize list to store tweet counts[clinton, trump, sanders, cruz] = [0, 0, 0, 0]# Iterate through df, counting the number of tweets in which# each candidate is mentionedfor index, row in df. iterrows():  clinton += word_in_text('clinton', row['text'])  trump += word_in_text('trump', row['text'])  sanders += word_in_text('sanders', row['text'])  cruz += word_in_text('cruz', row['text']) Tiếp theo ta sẽ tạo list trong Python, mỗi item sẽ có giá trị đếm bắt đầu =0. Mục đích ở đây là để đếm số tweet count được cho mỗi keyword.  Sử dụng vòng lặp để đi từng row và check, nếu gặp keyword sẽ +1 vào biến đếm. Nếu True (nghĩa là có keyword đó) thì += 1. Basic Data visualization# Import packagesimport seaborn as snsimport matplotlib. pyplot as plt# Set seaborn stylesns. set(color_codes=True)# Create a list of labels:cdcd = ['clinton', 'trump', 'sanders', 'cruz']# Plot the bar chartax = sns. barplot(cd, [clinton, trump, sanders, cruz])ax. set(ylabel= count )plt. show() Đầu tiên cần import 2 package như trên để vẽ biểu đồ Hàm sns. barplot() có 2 tham số:     Tham số đầu: list label cần biểu diễn giá trị   Tham số thứ 2: list chứa giá trị của các label cần biểu diễn. List này đã được khởi tạo và đếm ở code trước đó.    "
    }, {
    "id": 16,
    "url": "https://tuyen-nnt.github.io/python-basic/",
    "title": "Python - Part 1",
    "body": "2021/07/11 - I. LIST:  Thêm phần tử cho list:Chỉ cần + [list] x = [ a ,  b ,  c ,  d ]y = x + [ e ,  f ] Xóa phần tử list:Dùng del(phần tử). Lưu ý là khi xóa thì các phần tử ở sau bị đẩy index lên. x = [ a ,  b ,  c ,  d ]del(x[1]) Dấu [index:index] trong list     x[start:end] Với start là chỉ số lấy end chỉ số không lấyVí dụ:   x[2:5] : lấy giá trị của index từ 2 đến 4   x[:3] : từ đầu đến index 2   x[3:] : từ index 3 đến hết   x[:] : lấy hết phần tử    Dấu ;:Dùng để tách command code trên cùng 1 dòng, nếu khác dòng thì không cần. ```  Same line  command1; command2  Separate linescommand1command2 * Copy list:Khi copy kiểu ``x=y`` thì thực tế ta đang copy địa chỉ list của y cho x. Nghĩa là khi ta thay đổi phần tử trong x thì y cũng thay đổi theo. Để xử lý tình huống này, nếu chỉ muốn copy giá trị list thì ta dùng: ``x = list(y)`` hoặc ``x = y[:]``* Convert datatypex = str(y)x = int(y)x = float(y)&gt; Check datatype bằng function ``type()``* Xem cấu trúc của 1 function có sẵn:``help(max)`` hoặc ``?max``#### II. METHOD* Cũng là function nhưng dành cho từng type* Tất cả mọi thứ trong Python đều là object. * Object có các method riêng, phụ thuộc vào data type #### III. NumpyTại sao dùng Numpy?* Rất quyền lực, có thể sử dụng cho nhiều data type khác nhau. Tuy nhiên mỗi tập hợp (array) chỉ được chứa 1 loại data type. * Có thể thêm, xóa, sửa* Quan trọng trong Data Science	* Làm các phép toán cho các tập hợp	* Tốc độ nhanh* Ta thấy nếu áp dụng phép toán như -*/ trên kiểu dữ liệu list thì sẽ throw Error ``không hỗ trợ``. Nếu + thì sẽ ghép 2 list lại thành 1 list. &gt; Tuy nhiên, ta đang cần +-*/ trên 2 list theo index tương ứng. Thì Numpy sẽ giúp ta giải quyết khó khăn này. Giải pháp Numpy có gì?* Python kiểu số. Có hàm ``np. mean()`` và ``np. median()`` rất phổ biến trong data science. * Thay thế cho Python List : kiểu dữ liệu ``Numpy Array``* Giúp tính toán trên toàn bộ array* Nhanh và dễ dàng* Cài đặt trong terminal: ``pip3 install numpy``Như vậy, để thực hiện các phép toán trên list, ta phải chuyển nó sang kiểu dữ liệu Numpy Array như sau:import numpy as npnp_height = np. array(height)np_weight = np. array(weight)bmi = np_weight / np_height ** 2 **Lưu ý quan trọng:*** Mỗi array chỉ được chứa 1 loại data type. * Type khác nhau thì hành vi của nó sẽ khác nhau. VD: 	* Phép + 2 list thì là ghép 2 list thành 1. 	* Phép + 2 array thì là cộng theo giá trị index tương ứng của 2 array với nhau. ##### 1. Numpy Subsetting * Ta có array ``bmi``. * Để truy cập vào array ta dùng cú pháp: ``bmi[index]``* Để xét các giá trị trong array có thỏa điều kiện không, ta dùng: ``bmi &gt; 23``&gt; Trả về array có kiểu Boolean (True/False)* Để trả về array chứa các giá trị thỏa điều kiện, ta dùng: ``bmi[bmi &gt; 23]`` ##### 2. Type của Numpy arrayNếu ``print(type(np_height))``&gt; numpy. ndarrayVới numpy là package, n là layer của array. ndarray là kiểu dữ liệu chỉ sử dụng trong Numpy. ##### 3. 2D Numpy Arrays Cách tạo 2D array bằng 2 array:* array1* array2* meas = np. array([array1, array2])Có thể xem 2D numpy array như phiên bản nâng cấp của  list của list  vì ta có thể thực hiện các phép toán với nó. Để tạo array 2D, ta chỉ cần input các giá trị vào như dưới đây, input 1 list có 2 sub-list vào argument của method ``np. array`` theo cấu trúc hình chữ nhật:![](/assets/images/2d-array. png)Mỗi sub-list là một row của array.  Nếu ta thay đổi kiểu dữ liệu của bất kỳ giá trị nào trong array sang kiểu khác như từ float sang string, thì mặc nhiên numpy sẽ chuyển tất cả các giá trị còn lại sang string (in ra sẽ thấy). &gt; Vì mỗi numpy array chỉ chứa 1 kiểu dữ liệu. Để biết cấu trúc data của array như thế nào, ta dùng attribute ``shape`` của array:![](/assets/images/array-shape. png) *Vì là attribute nên nó không có ``()`` như method~*Có 2 cú pháp để truy cập giá trị:![](/assets/images/2d-subset. png)* Cách 1: np_2d[row][column]* Cách 2: np_2d[row,column]* Dấu ``:`` vẫn sẽ được sử dụng giống như 1D array ở trên nếu muốn chọn cụ thể vùng giá trị muốn lấy. *Với row, column là index, chú ý ở đây vẫn sử dụng zero-index cho 2D array. *##### 4. Toán tử Boolean trong NumpyVới Numpy array ta có thể sử dụng phép so sánh như các ví dụ trên, nhưng nếu sử dụng kết hợp and, or, not thì sẽ throw Error. Do vậy, ta phải sử dụng:* np. logical_and()* np. logical_or() * np. logical_not()Ex:np. logical_and(my_house &gt; 13,        your_house &lt; 15) Kết quả trả về 1 Boolean Series, thích hợp dùng để filter dataframe```Với my_house và your_house là 2 Numpy array. "
    }, {
    "id": 17,
    "url": "https://tuyen-nnt.github.io/spark/",
    "title": "(ENG) The basics of Spark",
    "body": "2021/06/27 - Đầu tiên phải kết nối với Cluster. Cluster được host trên remote machine mà được connect với tất cả các node khác. Sẽ có 1 máy tính gọi là master phụ trách việc bóc tách dữ liệu và tính toán. master được kết nối với tất cả những máy tính còn lại trong cluster, các máy tính còn lại gọi là worker. master gửi cho workers data và các phép toán để chạy và chúng sẽ trả về cho master kết quả. The first step in using Spark is connecting to a cluster. In practice, the cluster will be hosted on a remote machine that’s connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master. Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like: Is my data too big to work with on a single machine?Can my calculations be easily parallelized?Bước 1: Create an instance of the SparkContext class to connect to a Spark cluster from PySpark. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you’re connecting to. An object holding all these attributes can be created with the SparkConf() constructor. Take a look at the documentation for all the details! You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection. from pyspark. sql import SparkSession, SQLContext# Create SparkSession. Creating multiple SparkSessions and SparkContexts can cause issues, so it's best practice to use the SparkSession. builder. getOrCreate() method. This returns an existing SparkSession if there's already one in the environment, or creates a new one if necessary!spark = SparkSession. builder. appName( CAR DAILY ). getOrCreate()# Create sparkContextsc = spark. sparkContextsqlContext = SQLContext(sc)# Verify SparkContextprint(sc)# Print Spark versionprint(sc. version)&lt;SparkContext master=local[*] appName=pyspark-shell&gt;You may also find that running simpler computations might take longer than expected. That’s because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. That means that for simple or small problems Spark may actually perform worse than some other solutions! Using DataFrames: Spark’s core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you’ll be using the Spark DataFrame abstraction built on top of RDDs. The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs. When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it’s up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in! To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection. I. SELECT flights. select(flights. air_time/60) returns a column of flight durations in hours instead of minutes. if you wanted to . select() the column duration_hrs (which isn’t in your DataFrame) you could do  flights. select((flights. air_time/60). alias(“duration_hrs”)) The equivalent Spark DataFrame method . selectExpr() takes SQL expressions as a string:  flights. selectExpr(“air_time/60 as duration_hrs”)with the SQL as keyword being equivalent to the . alias() method. To select multiple columns, you can pass multiple strings.  View tables:Once you’ve created a SparkSession, you can start poking around to see what data is in your cluster!Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information. One of the most useful is the . listTables() method, which returns the names of all the tables in your cluster as a list. spark. catalog. listTables() Querying:Running a query on this table is as easy as using the . sql() method on your SparkSession. This method takes a string containing the query and returns a DataFrame with the results!If you look closely, you’ll notice that the table flights dataframe is only mentioned in the query, not as an argument to any of the methods. This is because there isn’t a local object in your environment that holds that data, so it wouldn’t make sense to pass the table as an argument.  Convert Spark DF to Pandas DF - Pandafy a Spark DataFrameSuppose you’ve run a query on your huge dataset and aggregated it down to something a little more manageable. Sometimes it makes sense to then take that table and work with it locally using a tool like pandas. Spark DataFrames make that easy with the . toPandas() method. Calling this method on a Spark DataFrame returns the corresponding pandas DataFrame. It’s as simple as that! This time the query counts the number of flights to each airport from SEA and PDX. Remember, there’s already a SparkSession called spark in your workspace! # Don't change this queryquery =  SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest # Run the queryflight_counts = spark. sql(query)# Convert the results to a pandas DataFramepd_counts = flight_counts. toPandas()# Print the head of pd_countsprint(pd_counts. head())Convert from Pandas DF to Spark DF - Put some Spark in your dataIn the last exercise, you saw how to move data from Spark to pandas. However, maybe you want to go the other direction, and put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well. The . createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame. The output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can’t access the data in other contexts. For example, a SQL query (using the . sql() method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table. You can do this using the . createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you’d like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame. There is also the method . createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You’ll use this method to avoid running into problems with duplicate tables. Giải thích thêm: Trong 1 SparkSession có nhiều SparkContext như sql(),… Trong SparkSession nó có method giúp chuyển đổi từ Pandas DF thành Spark DF. Spark Dataframe tạo ra từ . createDataFrame() cho kết quả về được lưu locally nhưng không được lưu vào SparkSession catalog. Khi đó ta không thể truy cập từ các context khác nhau trong cùng SparkSession như không thể query . sql(). Do vậy sau khi dùng method trên, ta dùng . createTempView() là method dành cho Spark DF để đăng ký Spark DF này thành 1 temporary table trong catalog và có thể sử dụng chung cho các context khác nhau. Tuy nhiên, vì là temporary nên Spark DF này không dùng chung được cho các SparkSession khác nhau mà chỉ dùng cho Session mà tạo ra nó. Ngoài ra, . createOrReplaceTempView() tương tự method tạo temp table nhưng nó giúp tránh duplicate table nếu như nó đã có tạo rồi thì update.  # Create pd_temppd_temp = pd. DataFrame(np. random. random(10))# Create spark_temp from pd_tempspark_temp = spark. createDataFrame(pd_temp)# Examine the tables in the catalogspark. catalog. listTables()# Add spark_temp to the catalogspark_temp. createOrReplaceTempView( temp )# Examine the tables in the catalog againprint(spark. catalog. listTables())Xem cách các Spark data structure tương tác với nhau bằng nhiều cách trong biểu đồ dưới đây: Convert thành Spark DF trực tiếp từ file . csv mà không cần thông qua pandas DF:Now you know how to put data into Spark via pandas, but you’re probably wondering why deal with pandas at all? Wouldn’t it be easier to just read a text file straight into Spark? Of course it would! Luckily, your SparkSession has a . read attribute which has several methods for reading different data sources into Spark DataFrames. Using these you can create a DataFrame from a . csv file just like with regular pandas DataFrames! The variable file_path is a string with the path to the file airports. csv. This file contains information about different airports all over the world. # Don't change this file pathfile_path =  /usr/local/share/datasets/airports. csv # Read in the airports dataairports = spark. read. csv(file_path, header=True)# Show the dataairport. head()Creating columnsIn this chapter, you’ll learn how to use the methods defined by Spark’s DataFrame class to perform common data operations. Let’s look at performing column-wise operations. In Spark you can do this using the . withColumn() method, which takes two arguments. First, a string with the name of your new column, and second the new column itself. The new column must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using df. colName. Updating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can’t be changed, and so columns can’t be updated in place. Thus, all these methods return a new DataFrame. To overwrite the original DataFrame you must reassign the returned DataFrame using the method like so: df = df. withColumn( newCol , df. oldCol + 1)The above code creates a DataFrame with the same columns as df plus a new column, newCol, where every entry is equal to the corresponding entry from oldCol, plus one. To overwrite an existing column, just pass the name of the column as the first argument! # Create the DataFrame flightsflights = spark. table( flights )# Show the headflights. show()# Add duration_hrsflights = flights. withColumn( duration_hrs , flights. air_time/60)flights. show()Use the spark. table() method with the argument  flights  to create a DataFrame containing the values of the flights table in the . catalog. Save it as flights. Show the head of flights using flights. show(). Check the output: the column air_time contains the duration of the flight in minutes. Update flights to include a new column called duration_hrs, that contains the duration of each flight in hours (you'll need to divide duration_hrs by the number of minutes in an hour). Filtering DataNow that you have a bit of SQL know-how under your belt, it’s easier to talk about the analogous operations using Spark DataFrames. Let’s take a look at the . filter() method. As you might suspect, this is the Spark counterpart of SQL’s WHERE clause. The . filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values. For example, the following two expressions will produce the same output: flights. filter(“air_time &gt; 120”). show()flights. filter(flights. air_time &gt; 120). show() Notice that in the first case, we pass a string to . filter(). In SQL, we would write this filtering task as SELECT * FROM flights WHERE air_time &gt; 120. Spark’s . filter() can accept any expression that could go in the WHEREclause of a SQL query (in this case, “air_time &gt; 120”), as long as it is passed as a string. Notice that in this case, we do not reference the name of the table in the string – as we wouldn’t in the SQL request. In the second case, we actually pass a column of boolean values to . filter(). Remember that flights. air_time &gt; 120 returns a column of boolean values that has True in place of those records in flights. air_time that are over 120, and False otherwise. # Filter flights by passing a stringlong_flights1 = flights. filter( distance &gt; 1000 )# Filter flights by passing a column of boolean valueslong_flights2 = flights. filter(flights. distance &gt; 1000)# Print the data to check they're equallong_flights1. show()long_flights2. show()=&gt; Kết quả như nhau SelectingThe Spark variant of SQL’s SELECT is the . select() method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df. colName syntax). When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside . withColumn(). The difference between . select() and . withColumn() methods is that . select() returns only the columns you specify, while . withColumn() returns all the columns of the DataFrame in addition to the one you defined. It’s often a good idea to drop columns you don’t need at the beginning of an operation so that you’re not dragging around extra data as you’re wrangling. In this case, you would use . select() and not . withColumn(). Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. # Select the first set of columnsselected1 = flights. select( tailnum ,  origin , dest )# Select the second set of columnstemp = flights. select(flights. origin, flights. dest, flights. carrier)# Define first filterfilterA = flights. origin ==  SEA # Define second filterfilterB = flights. dest ==  PDX # Filter the data, first by filterA then by filterBselected2 = temp. filter(filterA). filter(filterB)Select the columns “tailnum”, “origin”, and “dest” from flights by passing the column names as strings. Save this as selected1. Select the columns “origin”, “dest”, and “carrier” using the df. colName syntax and then filter the result using both of the filters already defined for you (filterA and filterB) to only keep flights from SEA to PDX. Save this as selected2. Selecting IISimilar to SQL, you can also use the . select() method to perform column-wise operations. When you’re selecting a column using the df. colName notation, you can perform any column operation and the . select() method will return the transformed column. For example, flights. select(flights. air_time/60) returns a column of flight durations in hours instead of minutes. You can also use the . alias() method to rename a column you’re selecting. So if you wanted to . select() the column duration_hrs (which isn’t in your DataFrame) you could do flights. select((flights. air_time/60). alias( duration_hrs )) The equivalent Spark DataFrame method . selectExpr() takes SQL expressions as a string: flights. selectExpr( air_time/60 as duration_hrs ) with the SQL as keyword being equivalent to the . alias() method. To select multiple columns, you can pass multiple strings. Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. # Define avg_speedavg_speed = (flights. distance/(flights. air_time/60)). alias( avg_speed )# Select the correct columnsspeed1 = flights. select( origin ,  dest ,  tailnum , avg_speed)# Create the same table using a SQL expressionspeed2 = flights. selectExpr( origin ,  dest ,  tailnum ,  distance/(air_time/60) as avg_speed )AggregatingAll of the common aggregation methods, like . min(), . max(), and . count() are GroupedData methods. These are created by calling the . groupBy() DataFrame method. You’ll learn exactly what that means in a few exercises. For now, all you have to do to use these functions is call that method on your DataFrame. For example, to find the minimum value of a column, col, in a DataFrame, df, you could do df. groupBy(). min( col ). show() This creates a GroupedData object (so you can use the . min() method), then finds the minimum value in col, and returns it as a DataFrame. Now you’re ready to do some aggregating of your own! A SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. Ex: flights. show()# Find the shortest flight from PDX in terms of distanceflights. filter(flights. origin ==  PDX ). groupBy(). min( distance ). show()# Find the longest flight from SEA in terms of air timeflights. filter(flights. origin ==  SEA ). groupBy(). max( air_time ). show()Grouping and Aggregating IPart of what makes aggregating so powerful is the addition of groups. PySpark has a whole class devoted to grouped data frames: pyspark. sql. GroupedData, which you saw in the last two exercises. You’ve learned how to create a grouped DataFrame by calling the . groupBy() method on a DataFrame with no arguments. Now you’ll see that when you pass the name of one or more columns in your DataFrame to the . groupBy() method, the aggregation methods behave like when you use a GROUP BY statement in a SQL query! # Group by tailnumby_plane = flights. groupBy( tailnum )# Number of flights each plane madeby_plane. count(). show()# Group by originby_origin = flights. groupBy( origin )# Average duration of flights from PDX and SEAby_origin. avg( air_time ). show()Create a DataFrame called by_plane that is grouped by the column tailnum. Use the . count() method with no arguments to count the number of flights each plane made. Create a DataFrame called by_origin that is grouped by the column origin. Find the . avg() of the air_time column to find average duration of flights from PDX and SEA. Grouping and Aggregating IIIn addition to the GroupedData methods you’ve already seen, there is also the . agg() method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the pyspark. sql. functions submodule. This submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table. # Import pyspark. sql. functions as Fimport pyspark. sql. functions as F# Group by month and destby_month_dest = flights. groupBy( month ,  dest )# Average departure delay by month and destinationby_month_dest. avg( dep_delay ). show()# Standard deviation of departure delayby_month_dest. agg(F. stddev( dep_delay )). show()Import the submodule pyspark. sql. functions as F. Create a GroupedData table called by_month_dest that's grouped by both the month and dest columns. Refer to the two columns by passing both strings as separate arguments. Use the . avg() method on the by_month_dest DataFrame to get the average dep_delay in each month for each destination. Find the standard deviation of dep_delay by using the . agg() method with the function F. stddev(). JoiningIn PySpark, joins are performed using the DataFrame method . join(). This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, on, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table. The third argument, how, specifies the kind of join to perform. In this course we’ll always use the value how=”leftouter”. # Examine the dataprint(airports. show())print(flights. show())# Rename the faa columnairports = airports. withColumnRenamed( faa ,  dest )# Join the DataFramesflights_with_airports = flights. join(airports, dest ,how= leftouter )# Examine the new DataFrameprint(flights_with_airports. show())Examine the airports DataFrame by calling . show(). Note which key column will let you join airports to the flights table.   Rename the faa column in airports to dest by re-assigning the result of airports. withColumnRenamed(“faa”, “dest”) to airports.   Join the flights with the airports DataFrame on the dest column by calling the . join() method on flights. Save the result as flights_with_airports.     The first argument should be the other DataFrame, airports.     The argument on should be the key column.     The argument how should be “leftouter”.   Call . show() on flights_with_airports to examine the data again. Note the new information that has been added. Machine Learning Pipeline: Learn more at: http://spark. apache. org/docs/2. 1. 0/api/python/pyspark. html "
    }, {
    "id": 18,
    "url": "https://tuyen-nnt.github.io/data-visualization/",
    "title": "Data Visualization",
    "body": "2021/06/23 - 1. Các loại chart cơ bản: BAR CHART &amp; COLUMN CHART  Giúp ta nhìn vào giá trị cụ thể cho mỗi loại Có 4 loại:  Stacked bar và column chart     Biểu đồ chồng nhau theo giá trị,     Clustered bar và column chart     Biểu đồ nhiều cột trong 1 phân loại     100% stacked bar và column chart     Biểu đồ chồng theo %     Combo chart     Biểu đồ kết hợp cột và biểu đồ đường    LINE CHART  Giúp biểu diễn chuỗi giá trị theo dạng có hình, thường là thông qua diễn tiến thay đổi của thời gian AREA CHART  Dựa trên Line chart nhưng được fill màu PIE CHART &amp; DONUT CHART  Biểu diễn mỗi quan hệ giữa các thành phần và tổng thể TREE MAP  Biểu diễn mỗi quan hệ giữa các thành phần và tổng thể, với các hình vuông có màu có kích cỡ riêng biểu thị phần mà các giá trị chiếm. CARD &amp; MULTI-ROW CARD  Card     biểu diễn 1 giá trị    *Multi-row card  dùng để biểu diễn nhiều giá trị GAUGE CHART &amp; KPI  được thiết kế để hiển thị dữ liệu thực tế so sánh với dữ liệu ngân sách/doanh thu hoặc mục tiêu đã lên kế hoạch. TABLE &amp; MATRIX  biểu diễn chi tiết dữ liệu văn bản bằng định dạng Bảng  Table:     chứa dữ liệu liên quan trong chuỗi logical của dòng và cột, có thể bao gồm header &amp; footer của bảng     Matrix:     giống như bảng nhưng matrix có thể thu lại hoặc mở rộng ra bằng dòng hoặc cột    HIERARCHIES:  Level data từ cao đến thấp Ví dụ:Year =&gt; Quarter =&gt; Month =&gt; DayCompany =&gt; Region =&gt; Country =&gt; DIvision =&gt; Unit HÀM:  Là các công thức được định nghĩa trước sẵn để biểu diễn các phép tính trên các giá trị gọi là tham trị arguments. 2. Kiến thức nền tảng để visualize data: Có 3 cách để lấy được insight của data:    Cách 1: Tính toán thống kêmean (trung bình), median(trung vị), standard deviation (phương sai)     Cách 2: Run model/Chạy mô hìnhLinear (Tuyến tính) hoặc hồi quy logistic     Cách 3: Vẽ plotscatter, bar, histogram (biểu đồ tần suất),…Scatter plot:  The Datasaurus Dozen  Có 13 datasets, mỗi Dataset có 2 trục x và y được gọi là variable Variable đơn giản chỉ là biệt ngữ thống kê để chỉ cột dữ liệu Khi tính phương sai của mỗi dataset, ta sẽ tính ra 2 phương sai cho x và y trên tất cả các record. Vì dataset trên có 2 giá trị x và y.  Phương sai dùng để tính toán sự biến thiên của dữ liệu. Chọn biểu đồ chấm plot loại gì thì phù hợp?Trước tiên phải xác định variable x và y của data thuộc loại nào. Có 3 loại:    Continuous : thường là số và có thể làm các phép toán cho nó. Ví dụ như nhiệt độ, chiều cao, doanh thu,…     Categorical : thường là dạng văn bản text, những thứ được phân loại hay mô tả. Ví dụ như màu mắt, quốc gia,…     Cả hai loại trênVí dụ như tuổi thì dạng continuous, còn nhóm tuổi từ 25-30 thì lại là categorical. Thời gian thì continuous còn tháng thì categorical.      Ở đây tùy vào mục đích visualization mà bạn sẽ quyết định nó thuộc loại nào cho phù hợp.    Khi nào nên dùng biểu đồ tần suất Histogram? Nếu bạn có biến continuous như trên Khi bạn muốn biết hình dạng của sự phân tán data, ví dụ như bạn muốn biểu đồ thể hiện rõ giá trị cao nhất và thấp nhất. Một số thuật ngữ:  Bin (interval) : khoảng cách các ô trục trên biểu đồ, ví dụ 0-5, 5-10,… Nếu muốn rõ data hơn thì ta co bin lại 0-1, 1-2,…Như hình, bin 10-15 tuổi có giá trị trục y là 4, nghĩa là có 4 người từ 10-15 tuổi. Sự lựa chọn binwidth sẽ ảnh hưởng lớn đến hình ảnh biểu đồ. Dưới đây cho thấy nếu thu nhỏ bin lại còn 1 năm tuổi thì nhìn rất lộn xộn.  Các tiêu chí trải nghiệm qua để chọn binwidth phù hợp:    Modality : có bao nhiêu đỉnh trong biểu đồ?Unimodal, bimodal hay trimodal?     Skewness: lệch lạc hay cân xứng?lệch trái, phải hay cân đối ở giữa?     Kurtosis: có bao nhiêu điểm có giá trị = 0? (extreme value)  Khi nào nên vẽ Box Plots? Khi chúng ta có 1 variable là continuous, được tách ra phân loại bởi 1 variable categorical.  Khi chúng ta muốn so sánh sự phân tán dữ liệu của variable continuous cho mỗi category (phân loại). Các chỉ số trên box plots:  lower quartile: 1/4 có giá trị dưới số này median : trung vị upper quartile: 1/4 có giá trị trên số này inter-quartile range: khoảng cách từ lower đến upper whiskers: đường thẳng ngang hai bên, đường kẻ ra gấp 1-1,5 lần inter-quarter, dài đến mức độ đủ để biết rằng ngoài đường kẻ thì không có giá trị nào cả (extreme value).  Phân loại nào chỉ có đường thẳng nghĩa là chỉ có 1 giá trị. "
    }, {
    "id": 19,
    "url": "https://tuyen-nnt.github.io/phpmyadmin/",
    "title": "Set-up phpMyAdmin",
    "body": "2021/06/15 -    Initial setup with Ubuntu 20. 4: https://www. digitalocean. com/community/tutorials/initial-server-setup-with-ubuntu-20-04     Install Mysql on Ubuntu 20. 4:https://www. digitalocean. com/community/tutorials/how-to-install-mysql-on-ubuntu-20-04     Install LAMP (Linux, Apache, MySQL, PHP)https://www. digitalocean. com/community/tutorials/how-to-install-linux-apache-mysql-php-lamp-stack-on-ubuntu-20-04     Install and Secure phpMyAdmin:https://www. digitalocean. com/community/tutorials/how-to-install-and-secure-phpmyadmin-on-ubuntu-20-04     Secure Apache with Let’s Encrypt on Ubuntu 20. 04 (free TLS/SSL certificate)https://www. digitalocean. com/community/tutorials/how-to-secure-apache-with-let-s-encrypt-on-ubuntu-20-04     DOCKER COMPOSE &amp; NETWORKhttps://vsudo. net/blog/docker-toan-tap. html  CÓ 2 CÁCH ĐỂ SỬ DỤNG phpMyAdmin:C1: cài đặt thủ công và sử dụng trên localhost hoặc domain trên server mình host (xem link trên)https://www. digitalocean. com/community/tutorials/how-to-install-and-secure-phpmyadmin-on-ubuntu-20-04 =&gt; đã thử và okC2:dùng Docker network chứa 2 container là MySQL và phpMyAdmin để truy cập MySQL server trên phpMyAdmin hosthttps://vsudo. net/blog/docker-toan-tap. html =&gt; đã thử và ok Cách để đưa db của mình chạy trên container có sẵn của phpMyAdminB1: tạo image chạy trên base php cho file . php chứa truy vấn bảng của db (như 1 ứng dụng) và phải cài đặt biến môi trường trong này cho giống với config của container. B2: compose với container của phpMyAdmin có sẵn (có thể chọn các image phù hợp nhu cầu) và bổ sung image ở trên vào file docker-compose. yml --------------------------------------------------WORKDIR /var/www/tuyen. techCOPY /etc/apache2/sites-available/tuyen. tech. conf /etc/apache2/sites-available/COPY /etc/apache2/conf-available/phpmyadmin. conf /etc/apache2/conf-available/COPY /usr/share/phpmyadmin/. htaccess /usr/share/phpmyadmin/COPY . /var/www/tuyen. tech/RUN sudo apt update &amp;&amp; apt install -y \&amp;&amp; apache2 \&amp;&amp; php libapache2-mod-php php-mysql \&amp;&amp; mysql-server \&amp;&amp; phpmyadmin php-mbstring php-zip php-gd php-json php-curl \&amp;&amp; phpmyadmin RUN sudo mysql_secure_installation \&amp;&amp; sudo mysql \&amp;&amp; sudo phpenmod mbstring \&amp;&amp; sudo htpasswd -c /etc/phpmyadmin/. htpasswd tuyen \&amp;&amp; sudo systemctl restart apache2 EXPOSE 9090--------------------------FROM ubuntu:latestMAINTAINER tuyennnt &lt;tuyendev96@gmail. com&gt;WORKDIR /var/www/tuyen. techCOPY /etc/apache2/sites-available/tuyen. tech. conf /etc/apache2/sites-available/COPY /etc/apache2/conf-available/phpmyadmin. conf /etc/apache2/conf-available/COPY /usr/share/phpmyadmin/. htaccess /usr/share/phpmyadmin/COPY . /var/www/tuyen. tech/RUN sudo apt update &amp;&amp; apt install -y \&amp;&amp; apache2 \&amp;&amp; php libapache2-mod-php php-mysql \&amp;&amp; mysql-server \&amp;&amp; phpmyadmin php-mbstring php-zip php-gd php-json php-curl \&amp;&amp; phpmyadmin RUN sudo mysql_secure_installation \&amp;&amp; sudo mysql \&amp;&amp; sudo phpenmod mbstring \&amp;&amp; sudo htpasswd -c /etc/phpmyadmin/. htpasswd tuyen \&amp;&amp; sudo systemctl restart apache2 EXPOSE 9090: : "
    }, {
    "id": 20,
    "url": "https://tuyen-nnt.github.io/data-etl/",
    "title": "Extract - Transform - Load (ETL process)",
    "body": "2021/05/29 - Tìm hiểu về ETLI. EXTRACT DATA: 1. Các cách để trích xuất dữ liệu: Có 3 cách:    Trích xuất từ file text, như file . txt or . csvText file có 3 loại là : 	* Text thuần	* Flat file (là file có . tsv hoặc . csv cách nhau bởi dấu “,” hoặc “tab” giữa các giá trị). Những file này có dòng thể hiện các record và cột thể hiện attribute của record. 	*File JSON: bán cấu trúc, có 4 kiểu dữ liệu atomic là number, string, boolean và null và 2 kiểu dữ liệu dạng composite là array và object. 	JSON có package hỗ trợ là “json” để import data	     Trích xuất từ web hoặc APIs của web services, như là Hacker News API      Thông qua web:      Ví dụ: bạn tìm kiếm thông tin trên google. com thì trình duyệt của bạn sẽ gửi request của bạn đến server của google và google sẽ trả về dữ liệu mà bạn đang tìm kiếm. * Thông qua API của web:Không phải lúc nào các trang web cũng trả về kết quả mà người thường có thể đọc ngay, mà các trang web đó sẽ trả về định dạng JSON thông qua API mà chúng ta request. Ta xem ví dụ request API từ trang Hackernews:Ta import package “request” rồi dùng method . get() để chèn vào link web cần lấy file JSON. Sau đó ta dùng method . json() để phân tích file JSON từ kết quả đã lấy được và chuyển hóa nó thành Python object.  Trích xuất từ một database trên web services Hầu hết các ứng dụng web đều có database để backup và để không bị ảnh hưởng khi tắt server, v. v. Cần phân biệt 2 loại database chính trong trường hợp này:     Application database : dùng cho trường hợp có nhiều giao dịch được cập nhật, loại này còn có tên gọi là OLTP (online transaction processing)   Analytical database: được xây dựng cho việc phân tích dữ liệu còn gọi là OLAP (online analytical processing)   2. Trích xuất dữ liệu từ database như thế nào?:  Dùng URI/chuỗi connection, cú pháp như sau:  [database_type]://[user[:password]@][host][:port]  Trong Python, ta dùng connection URI thông qua package sqlalchemy để tạo database engine : Từ engine đã được tạo ra, ta có thể dùng nó để đặt vào 1 số package hỗ trợ nó tương tác với database, đặc biệt là package pandas.  import requests```Lấy dữ liệu từ bài viết của Hackernews về, F12 inspect để lấy URL của nóresp = requests. get(“https://hacker-news. firebaseio. com/v0/item/16222426. json”) in dữ liệu vừa parse thành file json ra màn hìnhprint(resp. json()) parse dữ liệu ra rồi gán value của key “score” vào biến post_score, sau đó in cái biến rapost_score = resp. json()[“score”]print(post_score) * Một số ví dụ mở rộng hơn (xem phần 3 để hiểu hơn):Đọc dữ liệu từ database của postgreSQL, hàm extract dùng SQL query có nhiệm vụ chuyển từ dữ liệu bảng thành kiểu object mà pandas dùng (là dataframe)Function to extract table to a pandas DataFramedef extract_table_to_pandas(tablename, db_engine):  query = “SELECT * FROM {}”. format(tablename)  return pd. read_sql(query, db_engine) Connect to the database using the connection URI, sử dụng package sqlalchemyconnection_uri = “postgresql://repl:password@localhost:5432/pagila” db_engine = sqlalchemy. create_engine(connection_uri) Extract the film table into a pandas DataFrame, lưu ý nhớ để tên bảng dạng chuỗiextract_table_to_pandas(“film”, db_engine) Extract the customer table into a pandas DataFrameextract_table_to_pandas(“customer”, db_engine)``` 2. TRANSFORM DATA: 1. Một số phương thức chuyển đổi dữ liệu: Có thể thực hiện 1 hoặc nhiều các hình thức trong giai đoạn chuyển đổi dữ liệu đã rút trích:  Select 1 hay nhiều cột Phiên dịch dữ liệu thành code. Ví dụ như New York sẽ biến thành NY Kiểm tra dữ liệu có đúng không, nếu dữ liệu không đúng với kiểu dữ liệu hoặc dữ liệu muốn nhận từ cột, ta có thể bỏ record đó đi. Ví dụ như cột ngày nhưng lại chứa giá trị khác ngày.  Tách dữ liệu của 1 cột thành nhiều cột Join dữ liệu từ các nguồn, các bảng khác nhau. 2. Một số ví dụ: Bạn có thể dùng package pandas để chuyển đổi dữ liệu nếu lượng dữ liệu nhỏ. Ta có ví dụ tách dữ liệu từ 1 cột thành 2 cột sử dụng pandas: Nếu dữ liệu lớn, thông thường người ta sẽ dùng PySpark. Ta có ví dụ chuyển đổi dữ liệu bằng cách join các bảng với nhau. Nhưng trước hết chúng ta cần đẩy dữ liệu lên Spark: jbdc:để nhắn nhủ với Spark là phải dùng JBDC để kết nối, sau đó, ta input vào tên của bảng và cuối cùng trong properties chúng ta đặt thông tin kết nối vào. Dưới đây là 2 bảng cần join với nhau thông qua customer_id để tính rating trung bình của mỗi customer dành cho các phim: Và làm sao để dùng PySpark join và tính toán dữ liệu? Xem ảnh dưới nhé: Mình sẽ cho ra thêm các bài viết tìm hiểu sâu hơn về PySpark trong thời gian tới, các bạn hãy cùng chờ đợi nhé! III. LOAD DATA: "
    }, {
    "id": 21,
    "url": "https://tuyen-nnt.github.io/react-native-css/",
    "title": "React Native - CSS rules",
    "body": "2021/05/24 - Phân biệt các attribute quan trọng sau để build giao diện trong react native:  flexDirection: “row” hoặc “column” justify-content : “center”main/primary axis phụ thuộc vào flexDirection, ta có space-evenly   align-items : “center”căn chỉnh secondary axis cho mỗi line, có các loại như center, baseline,…   flexWrap: “wrap” để các component ko bị chèn ép mất đi   align-Content : căn chỉnh toàn bộ nội dung của 1 block trên secondary axis, chỉ hoạt động khi có flexWrap.   align-self flex: 1 flexBasis: 100, //width (nếu main là row) or height, phụ thuộc vào primary axis (main) flexGrow: 1 //giống flex : 1, trải ra đầy màn hình   flexShrink: 1 //giống flex : -1, co lại để ko tràn màn hình     bottom, right, left, top : di chuyển qua trái, phải bao nhiêu pixel   position: “absolute” hoặc “relative” //giá trị mặc định trong react native là “relative”*Dimensions: import thư viện này trong react DIP công thức như sau: Physical Pixels = DIPs x Scale Factor vd điện thoại có scale là 2 và point điểm ảnh là 320x480 thì View đối với Width có giá trị 150 là 150x2=300 =&gt; nghĩa là bề ngang là 1/2 màn hình "
    }, {
    "id": 22,
    "url": "https://tuyen-nnt.github.io/docker/",
    "title": "Basics of Docker",
    "body": "2021/05/02 - Một số định nghĩa cơ bản:: Docker imageLà file chứa tất cả các thông tin của chương trình, kể cả system, dependency,… được đóng gói vào. Bạn có thể xem các image đã tạo trong folder /var/lib/docker/images kết hợp sudo tree &lt;path trên&gt; Cách diễn giải khác thì đây là một file bất biến - không thay đổi, chứa các source code, libraries, dependencies, tools và các files khác cần thiết cho một ứng dụng để chạy. Docker container:Là 1 môi trường tách biệt (như 1 container) được đặt trên máy ở 1 không gian riêng và không liên quan đến các chương trình khác. Cách diễn giải khác là một run-time environment mà ở đó người dùng có thể chạy một ứng dụng độc lập. Những container này rất gọn nhẹ và cho phép bạn chạy ứng dụng trong đó rất nhanh chóng và dễ dàng. Sự liên quan:   Image có thể tồn tại mà không cần container, trong khi container chạy thì cần có image đã tồn tại. Vì vậy, container phụ thuộc vào image và sử dụng nó để tạo ra run-time environment và chạy ứng dụng trên đó.     Cả image và container đều là thành phần quan trọng để tạo ra 1 running container. Docker image là cực kì quan trọng để chi phối và định hình 1 Docker container.  Cách RUN image trên UBUNTU: docker run ubuntu //nếu chưa pull ubuntu image về thì sẽ pull rồi start, còn nếu pull rồi thì nó sẽ start container này luôn =&gt; nếu khi start mà thấy chưa có interact gì thì nó sẽ stop container lại luôn docker ps //xem list các tiến trình đang chạy của container hoặc các container đang chạy docker ps -a //xem các container đã dừng lại docker run -it ubuntu // start container với interaction mode và load ubuntu image lên trong cái container này Các câu lệnh: history !2 //để output cmd thứ 2 trong list history ls -1 //líst ds theo dạng dọc ls -l //list ds chi tiet touch &lt;tên file&gt; //lệnh tạo file mkdir &lt;tên thư mục&gt; //lệnh tạo thư mục mv &lt;tên file hoặc địa chỉ&gt; &lt;tên file hoặc địa chỉ&gt; //đổi tên hoặc di chuyển cat &lt;tên file&gt; more &lt;tên file hoặc địa chỉ&gt; //xem nội dung theo từng page, bấm phím space để qua trang tiếp theo nhưng ko scroll up lên được, nhấn enter để chạy từng line để xem less &lt;tên file hoặc địa chỉ&gt; // cat &lt;tên file&gt; &gt; &lt;tên file khác&gt;//copy nội dung file này vào file khác cat &lt;tên file&gt; &lt;tên file khác&gt; &gt; &lt;tên combined file&gt;	//combine nội dung 2 file vào file khác dấu &gt; áp dụng cho hầu hết các câu lệnhvd: echo hello &gt; hello. txtls -l /etc &gt; hello. txtsẽ cho kết quả ra file hello. txt trên. Trong Linux, mọi thứ đều ở duới dangj file, từ process, devices, hoặc địa chỉ thư mục đều là files Cấu trúc Linux:  / bin: chưa các file binaries, chương trình boot: các file liên quan booting dev: devices etc: editable text configuration, chứa file config home: thư mục của user root: chỉ có root mới vào được thư mục này lib: libary files như software library dependencies var: như biến, chứa các file được update thường xuyên như log,… proc: chứa các tiến trình đang runningCác bước để sử dụng Docker:  Cài đặt Docker Tạo 1 program và Tạo file plain text Dockerfile ghi hướng dẫn để đưa cho docker đóng gói ứng dụng thành 1 image. Image chứa tất cả mọi thứ để chương trình chạy. IMAGE bao gồm:     A cut-down OS   Môi trường runtime (vd như Node, Python)   File ứng dụng   Thư viện thứ 3   Biến môi trường =&gt; Sau khi có cái Image này, ta sẽ nói với Docker để start container Container như 1 process, nhưng là process đăc biệt vì nó có file system được cung cấp bởi Image. Ứng dụng của chúng ta sẽ được chạy trong cái process/container này.    Ok giờ chúng ta start docker. Thay vì chạy ứng dụng như bình thường, ta gọi Docker để chạy bên trong cái container nhé docker run . . Ví dụ sự khác nhau : Để chạy 1 chương trình JS đơn giản, ta cần phải qua 4 bước:  Start OS Cài Node Copy file ứng dụng Chạy node app. jsVới docker, ta chỉ cần:    Bước 1:      Vào thư mục project trên vscode code . Tạo file Dockerfile không ext, và input các thông tin sau:   FROM node:alpine  Chọn image node trên trang docker hub và : distribution của linux, ở đây chọn alpine vì nó nhẹ nhất . COPY . /app  copy tất cả file execute vào thư mục app tạo mới, vào rồi copy vào trong image  hoặc có thể hiểu là trong cái image có file system, và chúng ta tạo thư mục app bên trong file system đó, trong thư mục app chứa tất cả các file của program.  File requirements. txt chứa các thư viện mà chương trình cần. Cần COPY trước khi RUN các cài đặt trong Dockerfile. VD của file:tornado==6. 1. Với Pycharm thì ta có thể dùng Tools=&gt;Sync Python Requirement để tự động generate. WORKDIR /app  tạo đường dẫn rút gọn CMD node /app. js  câu lệnh chạy ứng dụng, để docker biết mà chạy câu nào khi run  Bước 2: Ta nói docker đóng gói úng dụng thành image docker build -t hello-docker . trong đó, dấu . chỉ cho docker biết chỗ nào chứa Dockerfile hello-docker là tên repos docker mình tự tạo để quản lýĐể xem có bao nhiêu docker đang chạy: docker image ls kết quả câu lệnh có cột SIZE chỉ size của OS (alpine) + ứng dụng + node  Bước 3: Chạy thử docker image docker run hello-dockerĐây là câu run cơ bản, ta có thể chạy ở thư mục nào cũng được, vì image đã chứa mọi thứ cần. Hầu hết các docker run dùng cú pháp chuẩn này: Cách 1:: docker run -d -p 80:8888 shorten-link Với 8888 ở đây là listen port của program trên container tại đây https://github. com/tuyen-nnt/shortenLink/  Với 80 là port trên máy host của mình, -d là chạy ngầm. Sau khi docker run thì ta test container bằng cách chạy câu lệnh: docker exec -it &lt;CONTAINER ID&gt; bash Trả về: root@05b418a61ca3:/app# ls  ls để list ra các file trong docker image, có thể check xem bước COPY đã đúng những file mình cần để chạy chương trình chưa? Để test wep app trên browser, ta dùng địa chỉ ip của máy:port máy host (expose) và thêm /đường dẫn như cài đặt. Cách 2:: Không cần port mà dùng luôn localhost, nghĩa là user bên ngoài chỉ cần dùng IP gateway:port của chương trình là được: docker run -d --net=host shorten-linkTuy nhiên nên hạn chế cách này, vì sẽ khiến conflict với các chương trình khác nếu chẳng may host mình cũng dùng port 8888 này ở chương trình đâu đó, vì mình không quyết định và control được port map (interface) như cách 1 để tùy chỉnh. Còn để host kiểu này khó biết được.  Bước 4 Publish lên docker hub: hub. docker. comKhi ở máy khác, ta muốn pull image về chạy:    Bước 1: Kiểm tra version docker, nếu chưa download thì chạy lệnh install docker vesion     Bước 2: Kéo image về docker pull senrie/hello-docker  Kiểm tra lại = cách: docker image ls  Bước 3: Chạy chương trình trên machine docker run senrie/hello-dockerVề PORTS trong Docker: Ta có thể xem các container đã build bằng lệnh docker ps : danh sách container đang chạy, và lệnh docker ps -a: tất cả container đã build. Ta thấy có cột PORTS, ví dụ: 0. 0. 0. 0:80-&gt;8888/tcp, :::8888-&gt;8888/tcpMình sẽ giải thích từng chỉ số:  0. 0. 0. 0: nghĩa là có thể dùng tất cả các IP của máy (IP card mạng LAN, IP wifi,. . có thể check bằng ifconfig) để làm gateway cho bên ngoài truy cập vào thông qua port cụ thể bên dưới.  80 (layer ngoài): là port của máy host expose cho bên ngoài dùng để truy cập vào, như cánh cổng mở ra 8888 (layer trong): là listen port của chương trình trên container trong máy host của chúng ta.      Port 80 sẽ được setup trên tất cả các gateway và centralize lại để link với container có port 8888. Nghĩa là ta dùng IP gateway:80 thì sẽ tiếp cận được đến chương trình container.    Để check xem máy chúng ta đang chạy các port nào tránh conflict, ta dùng lệnh:sudo netstat -plunt Xem qua về Docker run để hiểu hơn:https://docs. docker. com/engine/reference/commandline/run/ Xem cấu hình: Các cmd thường dùng:  docker inspect &lt;containerID&gt; : xem các gateway, ip, network,… của container để biết nó đang xài network nào     Nếu cài port khi run thì network là “bridge” (khi netstat sẽ ra docker-proxy)   Nếu dùng –net=host thì network là “host”   …      wrk -t4 -c400 -d30s &lt;url&gt; : xem perform mance của web (tham khảo thêm tại https://github. com/wg/wrk):   Các bước fix bugs:   Bước 1: Xác định xem lỗi gì (dùng curl, Postman hoặc F12) Bước 2: Coi log. Ví dụ:  docker logs -f $(docker ps | grep  83-&gt;  | awk '{print $1}')  Thêm tag -f để xem log real time không cần in hết log.   Bước 3: Debug     Dynamic Analysis: dùng break point hoặc print ra console,…=&gt; Cách này áp dụng tốt cho các trường hợp phức tạp và đơn giản cũng ok nếu đoán được code chỗ nào lỗi.    Static Analysis: đọc code chay (khi không biết chỗ nào run function đó, thường thì trong hướng đối tượng hay gặp phải)   Tìm hiểu về Volumn trong Docker: "
    }, {
    "id": 23,
    "url": "https://tuyen-nnt.github.io/import-begin/",
    "title": "Import Data - Part 1",
    "body": "2021/04/10 - 1. Flat file:  Là file text thông thường chứa các record tổ hợp bởi các trường hoặc attribute (thuộc tính) và mỗi trường chứa nhiều nhất 1 thông tin.  Flat file chứa các row và mỗi row là 1 record.  Flat file chỉ dùng để chứa dữ liệu của 1 bảng tính.  Nó không có relationships như relational database Rất phổ biến trong data scienceCác loại flat file:  . csv . txt Có dấu phân cách , hoặc tab2. Các cách import Flat file: Có 2 loại package chính để import, tùy trường hợp sử dụng:  NumPy : nếu dữ liệu column cơ bản là số, vì:     Nó là lưu dữ liệu số dạng mảng một cách hiệu quả và nhanh   Nó cần thiết cho các package khác như scikit-learn sử dụng trong Machine-learning.    Nó có nhiều hàm hỗ trợ để dễ dàng import các mảng data số như loadtxt() và genfromtxt().     pandas : nếu dữ liệu column là cả số lẫn string, dataframe là dtype sinh ra để lưu dạng hỗn hợp này. Import flat file bằng NumPy# Import packageimport numpy as np# Assign filename to variable: filefile = 'digits. csv'# Load file as array: digitsdigits = np. loadtxt(file, delimiter=',')# Print datatype of digitsprint(type(digits)) Hàm np. loadtxt()tạo object numpy dạng mảng, cần 2 tham số là:     Tên file   Dấu phân cách    Chúng ta có thể tùy chỉnh bằng cách thêm các tham số khác vì NumPy có nhiều sự lựa chọn:```  Import numpy  import numpy as np  Assign the filename: filefile = ‘digits_header. txt’ Load the data: datadata = np. loadtxt(file, delimiter=’\t’, skiprows=1, usecols=[0,2]) Print dataprint(data) * Các tham số ở trên bao gồm:	* ``skiprows= n`` : không lấy data của n dòng đầu tiên để đưa vào mảng, vì có thể đó là header hoặc các record mà ta không muốn lấy. 	* ``usecols=[]`` : tùy chọn những cột mà ta muốn lấy giá trị cách nhau bởi dấu phẩy, index côt bắt đầu từ 0 đến n-1 (với n là số côt). 	* ``print(array_object)`` : dùng để in object mảng ra console. 		Assign filename: filefile = ‘seaslug. txt’ Import file: datadata = np. loadtxt(file, delimiter=’\t’, dtype=str) Print the first element of dataprint(data[0]) Import data as floats and skip the first row: data_floatdata_float = np. loadtxt(file, delimiter=’\t’, dtype=float, skiprows=1) Print the 10th element of data_floatprint(data_float[9]) * Trong trường hợp set data của bạn có chứa các value với kiểu dữ liêu khác nhau ví dụ như giá trị dòng đầu tiên là header chứa string data. Có 2 cách để xử lý:	* *Cách 1*: Thêm tham số ``dtype=str`` để tất cả các giá trị khi import vào đều là string, và không bị báo lỗi ``ValueError``. 	* *Cách 2*: dùng tham số ``skiprows=n``, với n là số row sẽ skip từ row đầu tiên. Cách này nếu chúng ta biết được row nào có chứa giá trị khác các giá trị còn lại mà gây ra ``ValueError``. 	Plot a scatterplot of the dataplt. scatter(data_float[:, 0], data_float[:, 1])plt. xlabel(‘time (min. )’)plt. ylabel(‘percentage of larvae’)plt. show() * Sau khi có mảng data ta có thể tùy biến thành biểu đồ như mã code trên. #### 3. Import các loại file khác##### Excel spreadsheetsImport pandasimport pandas as pd Assign spreadsheet filename: filefile = ‘battledeath. xlsx’ Load spreadsheet: xlsxls = pd. ExcelFile(file) Print sheet namesprint(xls. sheet_names) * Lưu ý ở đây biến``xls`` mới chỉ là object Excel do pandas định nghĩa, chưa phải là dataframe. Vì excel có nhiều sheet nhưng 1 dataframe chỉ có thể là 1 sheet. Nên ta sẽ xử lý ở bước sau. * Hàm ``. sheet_names`` in ra tên tất cả các sheet trong bảng tính. Load a sheet into a DataFrame by name: df1df1 = xls. parse(‘2004’) Print the head of the DataFrame df1print(df1. head()) Load a sheet into a DataFrame by index: df2df2 = xls. parse(0) Print the head of the DataFrame df2print(df2. head()) * Hàm ``. parse()`` sẽ giúp rút trích df (sheet) của excel file và cần ta đưa vào tham số là tên của sheet hoặc chỉ số từ 0-(n-1), với n là số lượng sheet để load dataframe. ##### Pickled file (chuỗi byte hay bytestream =&gt; native trong Python)* Thực tế có những dạng datatype như dictionary hay list không có cách nào rõ ràng đưa vào lưu trong flat file như các datatype numpy. array hay pandas. dataframe. Do đó, pickle file ra đời. Đây là loại file dùng ngôn ngữ native mà con người đọc không hiểu. Nếu như bạn chỉ muốn import data thì chỉ cần *serialize* datatype dict hay list,. . . bằng cách convert nó sang dạng bytestream để trở thành pickled file. * Ở bài này, chúng ta chưa nói đến cách convert, mà sẽ học cách mở file pickled đã được convert sẵn và lưu ở local thay vì mở flat file như đã học trước đó. &gt; Có thể hiểu pickled file là file hỗ trợ bạn lưu dữ liệu có datatype kiểu dictionary, list,. . . ##### SAS7BDAT file ``. sas``SAS là viết tắt của Statistical Analysis System, dùng trong BA, BI, tính toán phân tích data hay thống kê về sinh học,. . . Import sas7bdat packagefrom sas7bdat import SAS7BDAT Save file to a DataFrame: df_saswith SAS7BDAT(‘sales. sas7bdat’) as file:  df_sas = file. to_data_frame() Print head of DataFrameprint(df_sas. head()) Plot histogram of DataFrame features (pandas and pyplot already imported)pd. DataFrame. hist(df_sas[[‘P’]])plt. ylabel(‘count’)plt. show() ##### Stata file ``. dta``Là sự kết hợp giữa statistics + data, dùng trong các nghiên cứu học thuật data về dịch tễ hay khoa học xã hội. import pandas as pddf = pd. read_stata(‘disarea. dta’) ##### HDF5Là loại file dùng để lưu trữ lượng data số lớn lên đến hàng trăm GBs hay TBs, thậm chí có thể scale lên ExabytesImport packagesimport numpy as npimport h5py Assign filename: filefile = ‘LIGO_data. hdf5’ Load file: datadata = h5py. File(file, ‘r’) Print the datatype of the loaded fileprint(type(data)) Print the keys of the filefor key in data. keys():  print(key) Get the HDF5 group: groupgroup = data[‘strain’] Check out keys of groupfor key in group. keys():  print(key) Set variable equal to time series data: strainstrain = data[‘strain’][‘Strain’]. value Set number of time points to sample: num_samplesnum_samples = 10000 Set time vectortime = np. arange(0, 1, 1/num_samples) Plot dataplt. plot(time, strain[:num_samples])plt. xlabel(‘GPS Time (s)’)plt. ylabel(‘strain’)plt. show()``` MATLAB . mateach row is an instance of entity type "
    }, {
    "id": 24,
    "url": "https://tuyen-nnt.github.io/csdl/",
    "title": "The origin of Database",
    "body": "2021/04/01 - Đi từ các cấu trúc dữ liệu như Array, Linked list, B-tree,… =&gt; Các cấu trúc dl này chỉ lưu những con số, không lưu được 1 tập dữ liệu liên quan với nhau =&gt; để lưu trữ tập hợp thông tin, người ta tạo ra CSDL =&gt; Để thêm xóa sửa nhanh chóng, người ta lại tạo ra ngôn ngữ SQL để làm phương tiện truy vấn dữ liệu Trừu tượng hóa dữ liệu cho phép mô tả dữ liệu theo  Đối tượng Thuộc tính Liên kết"
    }, {
    "id": 25,
    "url": "https://tuyen-nnt.github.io/ubuntu-basic/",
    "title": "Learn Linux commands on Ubuntu",
    "body": "2021/03/22 - Làm quen với Ubuntu1. Một số câu lệnh thường dùng: Part 1: Tải ứng dụng trên ubuntu: sudo apt-get install alacarte Uninstall ứng dụng: sudo apt-get remove alacarte Đổi tên file: mv [đường dẫn với tên cũ] [đường dẫn với tên mới] Ví dụ: Đổi tên tập tin test1. txt trong /root thành test. txt: mv /root/test1. txt /root/test. txtmv Di chuyển file:mv [đường dẫn nguồn] [đường dẫn đích] Ví dụ: Di chuyển và đổi tên tập tin *test1. txt trong /root sang /etc đổi tên thành test. txt: mv /root/test1. txt /etc/test. txt Mở file xem dùng lệnh:cat &lt;đường dẫn hoặc tên file trong thư mục hiện hành&gt; Edit file hoặc tạo file mới dùng lệnh:nano &lt;đường dẫn hoặc tên file trong thư mục hiện hành&gt; Tìm file trong thư mục hiện hành: find . [tên file]find . //list ra tất cả các file trong thư mụcCập nhật phiên bản, ví dụ npm 7. 13. 0: npm install -g npm@7. 13. 0 Mở OVPN: sudo openvpn --config Downloads/client. ovpn Mở ứng dụng sau khi cd vào folder chứa file thực thi, dùng lệnh sau để mở ứng dụng : . /datagrip. sh Tải Remarkable để viết markdown thông qua wget thay vì snap hoặc apt-get : wget https://remarkableapp. github. io/files/remarkable_1. 87_all. debsudo dpkg -i remarkable_1. 87_all. deb*Chạy đến đây nếu gặp lỗi  dpkg: dependency problems prevent configuration of remarkable  ta chạy câu lệnh*&gt; sudo apt-get install -fsudo apt-get install -fReset lai cau commit trước đó git reset --soft HEAD^^COveride commit đã push^C git push origin +masterPush lên brach master git push origin +master Part 2: update 23. 05. 2021: Xem lịch sử viết cmd:cat ~/. bash_history Giải nén với file taztar -xvzf . . .  Giải nén với file taz:tar -xvf . . .  Giải nén với file deb:sudo dpkg -i . . .  Để xem nó là dạng file gì :file . . . .  Xem là file hay thư mục và các quyền:ls -l Nếu cần quyền root để thực thi thì đổi sang quyền owner cho user sudo chown -R tuyen:tuyen . //thay đổi owner cho user, dấu  .   chỉ tất cả các file trong thư mục hiện hành, tuyen là user : tuyen là user groupchmod +x Để bổ sung cấu hình cho hệ thống: sudo nano hostsEx:# Một ứng dụng cần config&lt;ip&gt; www. XXX. com&lt;ip&gt; licxxxx. XXXX. comvoi ip la so dang XXX. X. X. X//Lưu ý dùng quyền sudo với những file của hệ thốngGiải nén file . deb dùng lệnh:sudo dpkg -i remarkable_1. 87_all. deb 2. Cài đặt server tải package apt-get về từ VN để nhanh hơn dùng mặc định của nước ngoài: cd /etc/aptlssudo cp sources. list sourceslist. bak//back-up filesudo sed -i 's/vn. archive. ubuntu. com/mirror. bizflycloud. vn/' sources. list//thay thế tất cả link /vn. archive. ubuntu. com = link mirror. bizflycloud. vn trong file sources. listcat sources. listsudo apt-get updatesudo rm sourceslist. bak 3. Compile ứng dụng từ source code thủ công: Đối với những ứng dụng dùng apt-get gặp vấn đề version hoặc không tương thích với máy, ta sẽ chuyển sang dùng cách này. Tuy nhiên cách này nếu may mắn sẽ compile nhanh, ngược lại có nhiều trường hợp thiếu thư viện khi . /configure dẫn đến mất nhiều thời gian. Đó là lí do nhiều người sẽ cài đặt Docker để compile từ source code nhanh =&gt; mình sẽ hướng dẫn ở phần sau.  B1: Download source code về từ git dùng git clonegit clone &lt;đường dẫn&gt; B2: cd vào thư mục ứng dụng B3: Cấu hình cho ứng dụng chạy trên máy bạn, ở bước này đòi hỏi sự kiên nhẫn để cài đặt thêm những thư viện cần, chú ý xem lỗi từ cmd để fix chính xác. . /configureKhi tải thêm thư viện thì cú pháp:  sudo apt-get install &lt;tên thư viện&gt;-dev//Lưu ý phải có -dev vì -dev là source code của mấy cái thư viện  B4: Compile từ source code thành file binary  make//Trong quá trình runtime nếu được yêu cầu tải thêm thư viện thì đuôi thư viện ko cần -dev nữa, vì các file config đã được compile ra binary hết rồi, ko còn là source code nữa =&gt; do vậy bỏ đuôi -dev để tương thích  B5: Install file binary vô hệ thống hay nói cách khác là chuyển những file đã được compile ở trên vô hệ thốngsudo make install4. Cách pin ứng dụng lên thanh dock ubuntu:  Bước 1: Tải file tar ứng dụng về và giải nén bằng lệnh tar -xvzf [tên_file] Bước 2: Tạo file tên ứng dụng có đuôi . desktop trong thư mục . local/share/applications, ví dụ webstorm. desktop bằng lệnh nano webstorm. desktop Bước 3: Mở file . desktop và paste dòng sau:# https://askubuntu. com/questions/975178/duplicate-icons-in-the-dock-of-gnome-shell-ubuntu-17-10/9752 30#975230# https://askubuntu. com/questions/990833/cannot-add-custom-launcher-to-dock-add-to-favorites# dconf-editor[Desktop Entry]Type=ApplicationTerminal=falseIcon[en_US]=/home/quocbao/Tools/WebStorm-2019. 2/bin/webstorm. svgName[en_US]=WebStormExec=/home/quocbao/Tools/WebStorm-2019. 2/bin/webstorm. shName=WebStormIcon=/home/quocbao/Tools/WebStorm-2019. 2/bin/webstorm. svgStartupWMClass=jetbrains-webstormTrong đó: * Icon: chứa đường dẫn thư mục có hình ảnh biểu tượng cho ứng dụng* Name: tên của ứng dụng* StartupWMClass: được lấy bằng cách chạy câu lệnh ``xprop WM_CLASS`` và bấm vào ứng dụng để hiện mã trên Terminal sau đó copy vào.  Bước 4: Gõ câu lệnh sudo apt-get install dconf-editor nếu chưa installTiếp theo gõ lệnh dconf-editor Sau đó chọn đến thư mục trong cửa số hiện lên:org/gnome/shell/favorite-apps Tại mục Customize value gõ tại vị trí mong muốn xuất hiện:  tên_file. desktop Lưu ý: các tên file cách nhau bởi dấu phẩy Một số lưu ý trong lúc thực hiện::  Khi file . sh không có quyền thực thi (kiểm tra quyền bằng câu lệnh ls -l &lt;tên_file&gt;, dùng tên file nếu đang ở trong thư mục chứa file còn không dùng đường dẫn đến file), ta cần thêm quyền thực thi (quyền mở file) bằng cách gõ câu lệnh sau:chmod +x &lt;đường dẫn file&gt;Tìm hiểu thêm các quyền tại đâyTìm hiểu thêm các lệnh khác tại đây Các loại đường dẫn:  ~/ : thay cho home /thư mục/… : là đường dẫn tuyệt đối, phải tuyệt đối chính xác, không rút gọn thư mục/…/ : là đường dẫn tương đối, đôi khi không có đoạn thư mục đầu vì đã ở trong thư mục đó rồi, v. v . / : chỉ thư mục hiện hành, nhưng đôi khi có các câu lệnh như “git . / bấm tiếp tab” sẽ gợi ý ra các file có thể áp dụng, nếu “git” thôi thì sẽ được list ra các câu lệnh có thể dùng với git, ví dụ . /vscode. shCre:    Part 1:https://askubuntu. com/questions/975178/duplicate-icons-in-the-dock-of-gnome-shell-ubuntu-17-10/975230#975230https://askubuntu. com/questions/990833/cannot-add-custom-launcher-to-dock-add-to-favoriteshttps://vinasupport. com/chmod-la-gi-huong-dan-su-dung-lenh-chmod-tren-linux-unix/https://blogd. net/linux/lam-viec-voi-tap-tin-va-thu-muc-tren-linux/     Part 2:https://www. liquidweb. com/kb/how-to-install-software-from-source-on-ubuntu/https://help. ubuntu. com/community/CompilingEasyHowTohttps://help. ubuntu. com/community/CompilingEasyHowTo  "
    }, {
    "id": 26,
    "url": "https://tuyen-nnt.github.io/data-toolbox/",
    "title": "Data Toolbox",
    "body": "2021/03/17 - Chapter 1 : Các công cụ làm việc trong mảng Data EngineeringI. Database: 1. Database là gì?: Database là tập hợp dữ liệu lớn được tổ chức đặc biệt để tìm kiếm và rút trích dữ liệu nhanh.  Các đặc điểm của nó:  Lưu trữ dữ liệu Tổ chức dữ liệu Rút trích/Tìm kiến dữ liệu thông qua DBMS2. Phân biệt dữ liệu có cấu trúc và dữ liệu không cấu trúc: Có cấu trúc:: Database có schema, ví dụ như các relational database Bán cấu trúc (semi-structured):: Ví dụ như file JSON { “key” : “value” }Không cấu trúc:: Không có schema, giống như các tập tin : videos, hình ảnh 3. Phân biệt SQL và NoSQL: SQL database có những đặc điểm sau::  Có các bảng để truy vấn Có Database schema (xác định sự liên hệ và thuộc tính của database) Có các Relational database (có quan hệ với nhau) Các hệ quản trị cơ sở dữ liệu phổ biến của ngôn ngữ này: MySQL, PostgreSQLNoSQL database có những đặc điểm sau::  Có các Non-relational database Thường gắn với các dữ liệu không có schema (unstructured), nhưng đôi khi có các dữ liệu có schema (structured) Lưu trữ các cặp key-value (e. g. dùng cho caching hoặc cấu hình phân tán “distributed configuration”) Những ứng dụng thường dùng NoSQL là những ứng dụng chứa cặp key-value như Redis hoặc MongoDB có model là document database. *Các giá trị trong document database thường là các object có cấu trúc hoặc bán cấu trúc, ví dụ như JSON object. 4. SQL: Tìm hiểu về Database chema: Database schema có vai trò gì?:  Có nhiệm vụ mô tả cấu trúc và các mối quan hệ của các bảng trong database thông qua SQL code và sơ đồ schema Star schema: Trong datawarehouse database, thường những schema chúng ta thấy là star schema có sơ đồ giống như hình ngôi sao. Fact table ở giữa và xung quanh là các Dimension tables, trong đó:  Fact table: là table chứa các dữ liệu xảy ra trên thế giới như product orders, v. v Dimension table: là các table mang thuộc tính để mô tả cho fact table như màu sắc, kích thước, v. vTìm hiểu thêm về Star chema tại đây Ví dụ thực tế:Giả sử ta có database mô tả cấu trúc dữ liệu cho hệ thống bán hàng ở 1 cửa hàng quần áo: Giả sử ta có bảng invoice lưu dữ liệu bán hàng (fact table) có các cột sau:       id (primary key)   created_date_time   items_id   payment_method   customer_id   card_id             123   12:00:00   12/12/2021   1   Bank   111   9116   Ta sẽ có các bảng dimension sau để bảng invoice liên kết mối quan hệ nhằm mô tả chi tiết các giao dịch invoice: Bảng Item       id   item_name   category   color   price         1   Quần đùi lửng   Quần   Đen   200000   Bảng Customer       id   name   birthday   card_id   is_valid   num_of_orders         111   Thanh Tuyền   09/XX/XXXX   9116   0   5   Bảng Card       id   customer_id   point   expired_date         9116   111   2100   31/12/2025   II. Parallel computing: Parallel computing là gì?: Trước tiên ta đi từ ý tưởng của việc tạo ra parallel computing.  Nó hình thành nên nền móng cho các công cụ xử lý data hiện đại (data processing tool) Tuy nhiên lí do quan trọng nhất phải nói đến là để tối ưu Bộ nhớ và hiệu năng xử lý dữ liệuCách thức hoạt động::  Chia task thành nhiều task phụ Phân tán task phụ trên một vài máy tính (thường là các commodity computer hoặc đã có sẵn để ít tốn phí thay vì sử dụng các siêu máy tính) Các máy tính làm việc song song với nhau trên các task phụ, do đó các task được hoàn thành nhanh hơnChúng ta hãy cùng xem ví dụ vận hành shop may đồ: Đặt mục tiêu may 100 cái áo, thì shop nhận thấy:  Nhân viên may giỏi nhất nhóm may 1 cái áo trong 20 phút =&gt; 12 cái áo trong 4 tiếng Những nhân viên khác thì may 1 cái áo trong 1 tiếngSau khi áp dụng mô hình parallel bằng cách:  Chia ra 4 chu kỳ Mỗi chu kì 25 cái áo và 4 nhân viên may làm việc song songThì ta thấy được nếu các nhân viên may có khả năng thấp hơn khi cùng làm việc song song với nhau thì sẽ làm được 16 cái áo trong 4 tiếng. Nói về thực tế máy tính, 4 nhân viên may có năng lực thấp hơn thì sẽ giống như là các máy tính có sẵn, ít tốn phí, thay vì ta phải thuê 1 nhân viên giỏi hay có thể nói là siêu máy tính thì chi phí sẽ cao hơn nhiều nhưng không chắc chắc hiệu quả bằng. Tóm lại, lợi ích của parallel computing:  Hiệu suất xử lý Bộ nhớ: chia dữ liệu thành các tập hợp con và đưa vào bộ nhớ của các máy tính khác nhau=&gt; mức chiếm dụng bộ nhớ tương đối nhỏ và dữ liệu được đưa vào trong bộ nhớ gần RAM nhẩtNhững rủi ro khi áp dụng: Overhead due to communication  Yêu cầu chưa đủ lớn để áp dụng Cần nhiều processing units hơn Đôi khi tách ra sẽ gặp vấn đề về program runtime, yêu cầu tốn nhiều thời gian để xử lý giao tiếp giữa các processes hơn so với không tách ra. Việc có nhiều chương trình hơn sẽ khiến thời gian contact switch giữa các chương trình nhiều hơn không phù hợp khi yêu cầu chưa đủ lớn. Xem hình bên dưới:Code Python để chia thành nhiều task phụ: Sử dụng API multiprocessing. Pool Sử dụng DASK framework để tránh viết API dưới hệ thống Một số ghi chú cho Parallel computing:    Không phải lúc nào cũng tăng thời gian xử lý công việc, do hiệu ứng của overhead communication ở phần contact switch giữa các ứng dụng.   GIúp tối ưu việc sử dụng bộ xử lý đa nhiệm (multiple processing unit)  Giúp tối ưu bộ nhớ giữa một số hệ thống Bài viết về Parallel computing đến đây là kết thúc, các bạn có thể xem qua 1 số đoạn code liên quan đến chủ đề này ở repo data-engineering của mình nhé! III. Parallel computation framework: 1. Hadoop: Hadoop là tập hợp các dự án open-source được maintain bởi Apache Software Foudation. Có 2 dự án phổ biến thường được nhắc đến là MapReduce và HDFS. a. Nói về HDFS: Đây là tên gọi của hệ thống phân tán tập tin (distributed file system). Cũng là các tập tin trong máy tính nhưng được điều đặc biệt là nó được phân tán trên nhiều máy tính khác nhau. HDFS là một phần thiết yếu trong Big data để lưu trữ dữ liệu lớn.  b. Nói về MapReduce: Đây là một trong các mô hình đầu tiên phổ biến trong việc xử lý Big data. Cách hoạt động của nó là chia task lớn thành nhiều task nhỏ rồi phân tán khối lượng dữ liệu và dữ liệu đến các đơn vị xử lý (processing unit). Tuy nhiên, MapReduce có một số khuyết điểm có thể kể đến như khó viết các job để chia task và phân tán chẳng hạn.  Chương trình phần mềm Hive và một só phần mềm khác ra đời để giải quyết khó khăn trên. Hive như là lớp vỏ trên cùng trong hệ sinh thái của Hadoop giúp các dữ liệu đến từ những nguồn khác nhau có thể truy vấn bằng cách sử dụng ngôn ngữ truy vấn có cấu trúc Hive SQL (HQL). Ví dụ đoạn truy vấn Hive SQL:  Nhận xét: trông chả khác câu lệnh SQL thông thường phải không ? ^^ Tuy nhiên sau tấm màn đó thì mọi thứ sẽ khác. Câu truy vấn trên sẽ chuyển đổi thành job có nhiệm vụ phân tán đến tập hợp các máy tính đấy (cluster). 2. Spark framework: Ngoài Hadoop, ta còn có Spark, cũng có nhiệm vụ phân tán các task xử lý dữ liệu giữa các cluster, ngày nay được sử dụng phổ biến hơn. Trong khi hệ thống Hadoop MapReduce cần ổ đĩa đắt tiền để ghi dữ liệu giữa các job, thì Spark lại sử dụng một cách tối ưu bộ nhớ xử lý tránh sử dụng ổ đĩa để ghi dữ liệu. Spark ra đời đã cho thấy những hạn chế của MapReduce, trong đó bao gồm việc MapReduce hạn chế tương tác của người dùng khi truy cập phân tích dữ liệu và mỗi bước build sẽ phải dựa trên bước trước đó. =&gt; không linh hoạt, khó tương tác hơn Spark. a. Kiến trúc của Spark: Dựa trên RDDs (Resilient distributed datasets) RDD là một loại cấu trúc dữ liệu có nhiệm vụ duy trì các dữ liệu được phân tán giữa nhiều node. Không giống với DataFrame, RDDs không có các cột. Về khái niệm thì có thể xem nó là một dãy các tuples, ví dụ về tuple “day”: day = ('monday', 'tuesday', 'wednesday' , 'thursday', 'friday', 'saturday' , 'sunday')Với các dữ liệu có cấu trúc RDD ta có thể thực thi 2 loại lệnh :  Chuyển đổi: dùng method . map() hoặc . filter() =&gt; output ra các dạng dữ liệu đã được chuyển đổi Hành động: dùng method . count() hoặc . first() =&gt; ra 1 output duy nhất (số, chữ, v. v)Spark framework có nhiều interface ứng với các ngôn ngữ lập trình, phổ biến nhất là PySpark dùng ngôn ngữ Python. Ngoài ra còn có các ngôn ngữ khác như Scala, R.  **PySpark dùng host dựa trên Dataframe trừu tượng, đó là lí do nếu chúng ta thường sử dụng thư viện pandas của dataframe sẽ dễ làm quen hơn vì hoạt động của PySpark sẽ tương tự thế.    Tóm lại là các công việc về parallel computing từ đơn giản đến phức tạp cứ để nhà Spark lo :smile: còn chaỵ như thế nào là tùy bạn.  Xem ví dụ về PySpark khi tính trung bình các vận động viên theo nhóm tuổi nhé: "
    }, {
    "id": 27,
    "url": "https://tuyen-nnt.github.io/one-day-just-started-adventure/",
    "title": "One day we just started our adventure",
    "body": "2020/01/12 - The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes. As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother.  It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story. The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter. An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”. "
    }, {
    "id": 28,
    "url": "https://tuyen-nnt.github.io/git-commit/",
    "title": "How to write effective git-commit?",
    "body": "2019/07/31 - Mẹo viết commit message hữu ích khi dùng gitGit là một hệ thống quản lý phiên bản phân tán phổ biến với mục đích theo dõi sự thay đổi của các tập tin trên máy tính và phối hợp công việc nhóm trên những tập tin này. Là một lập trình viên thì một trong những công cụ cơ bản mà chúng tôi không thể bỏ qua chính là việc sử dụng git trong quy trình làm việc. Sau đây là một trong những sai lầm mà dân lập trình hay mắc phải, về mặt kỹ thuật thì không hẳn nhưng chúng ta hãy tự hỏi rằng “Nếu bạn có thể làm tốt hơn thì tại sao không làm?” Một số quy tắc nên ghi nhớ khi viết commit message :    Tách dòng chủ đề ra khỏi phần thân bằng một dòng trống     Giới hạn dòng chủ đề tối đa 50 kí tự     Viết hoa dòng chủ đề     Sử dụng câu mệnh lệnh ở dòng chủ đề     Không kết thúc dòng chủ đề bằng dấu chấm     Gói gọn phần thân tối đa 72 kí tự     Sử dụng phần thân để giải thích “Cái gì”, “Tại sao” với “Như thế nào”  Sai lầm #1:: Chúng ta có khuynh hướng trộn lẫn chủ đề với phần thân của một commit message : Cách viết phần chủ đề chung với phần thân của thông điệp là một cách làm sai. Khi bạn nhận thấy commit message quá dài để giải thích thì có nghĩa là commit đang thực hiện quá nhiều thứ sẽ phá vỡ nó đi. Bạn viết:  git commit –m “added new css styling for danger button in order to differentiate between the primary button and other button styles. ” Thay vì nên viết:  git commit –m “Add new feature” Sai lầm #2:: Không giới hạn chủ đề tối đa 50 kí tự, phần thân của commit message tối đa 72 kí tự: Luôn đảm bảo chủ đề của commit không bao giờ vượt quá 50 kí tự và đây là qui tắc ngón tay cái. Thêm nhiều hơn số kí tự nên có sẽ có khuynh hướng sẽ bị Github cắt bớt đi, và vì những gì chúng ta đang cố gắng truyền tải là để một user nào đó trong nháy mắt biết được một commit đang làm gì. Một vài commit message yêu cầu giải thích nhiều hơn đặc biệt là khi dòng chủ đề có nội dung mơ hồ, do đó thêm nội dung phần thân sẽ hữu ích trong những trường hợp này. Luôn cố gắng giới hạn số lượng kí tự phần thân tối đa 72 kí tự, và hãy để phần thân giải thích những gì commit đang thực hiện và tại sao lại làm như vậy. Sai lầm #3:: Không viết hoa cho chủ đề của commit Bạn viết:  git commit –m “added new feature” Thay vì nên viết:  git commit –m “Add new feature” Sai lầm #4:: Kết thúc dòng chủ đề bằng dấu chấm Bạn viết:  git commit –m “Modified codebase. ”Thay vì nên viết: git commit –m “Modify codebase” Sai lầm #5:: Không sử dụng câu mệnh lệnh ở dòng chủ đề: Mọi git commit đúng chuẩn nên đặt ở dạng mệnh lệnh. “Đơn giản điều này có nghĩa là câu được viết theo dạng một hành động”. Một dòng chủ đề git commit thích hợp nên hoàn thành đúng cấu trúc cho câu sau đây:  If applied, this commit will “your subject line here” Ví dụ:  If applied, this commit will “Add auth to X” Từ đó, ta nhận thấy sẽ không đúng với những dạng không mệnh lệnh:  If applied, this commit will “added auth to Y” Sai lầm #6:: Phần thân giải thích Cái gì, Tại sao với Như thế nào: Như đã giải thích ở số #2 trên, luôn đảm bảo phần thân của commit giải thích chính xác cái gì và tại sao commit đó đang thực hiện. Giải thích tại sao sự thay đổi đó là cần thiết, và khía cạnh khác mà nó mang lại. Thay vì miêu tả cách mà bạn giải quyết vấn đề trên. Một số câu nói vui về git:  Why did the commit cross the rebase? To git to the other repoTạm dịch là: Sao commit qua được rebase? Vì ta git đến repo khác :v  In case of fire: git commit, git push, leave the buildingTạm dịch là: Khi gặp cháy thì phải git commit, git push, rồi mới rời khỏi tòa nhà :v Cre: https://code. likeagirl. io/useful-tips-for-writing-better-git-commit-messages-808770609503?fbclid=IwAR16siuZjdzxxQ4z9gz89BST5DPxINisBxzxVB-emDa-tuLl26KdkV15vsI Bài viết về git commit đến đây là hết rồi, mời các bạn xem thêm các bài viết khác nhé ! Tuyen Nguyen "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-primary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><small><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});